{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNh0o6CoJObk"
      },
      "source": [
        "# <font color='darkgreen'>บทที่ 1 - Machine Learning คืออะไร?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPaaLal8oNqV"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-builders/curriculum/blob/main/notebooks/01_ml_what.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5kizo28lXAt"
      },
      "source": [
        "ในบทเรียนนี้เราจะเรียนรู้ว่า Artificial Intelligence (AI), Machine Learning (ML) และ Deep Learning (DL) คืออะไร เหมือนกันหรือแตกต่างกันอย่างไร เราจะเรียนรู้ส่วนประกอบของระบบ machine learning และวิธีการเทรน machine learning model ด้วยตัวอย่างจำแนกรูปภาพอาหารไทย 48 ชนิดจากชุดข้อมูล [FoodyDudy](https://github.com/GemmyTheGeek/FoodyDudy) หลังจากนั้นเราจะเห็นว่าส่วนประกอบและวิธีการเทรนนี้ถูกใช้กับข้อมูลชนิดอื่นๆ เช่น ข้อความ (texts) และตาราง (tabular data) ได้อย่างไรบ้าง\n",
        "\n",
        "Checkpoint ท้ายบทประกอบด้วยคำถามชวนคิดเกี่ยวกับบทเรียน\n",
        "\n",
        "บทเรียนนี้ปรับแต่งและเพิ่มเติมจาก [fastai Practical Deep Learning for Coders v4 part1 - Lesson 1](https://course.fast.ai/videos/?lesson=1) และ [โครงการ AI Builders](https://github.com/ai-builders/curriculum)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ocsJNks3Jdx"
      },
      "source": [
        "## <font color='saddlebrown'>ติดตั้ง Package ที่ต้องใช้งาน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnVhBqci3QqR"
      },
      "outputs": [],
      "source": [
        "#ลง package ที่ต้องใช้; รันแค่ครั้งเดียวก็พอ\n",
        "!pip install pydantic\n",
        "!pip install scikit-image\n",
        "!pip install pandas\n",
        "!pip install statsmodels\n",
        "!pip install torch\n",
        "#pytorch ignite\n",
        "!pip install -q --pre pytorch-ignite==0.5.0.dev20230325\n",
        "\n",
        "#fastai\n",
        "!pip install -q fastbook==0.0.29\n",
        "\n",
        "#autogluon\n",
        "!pip install --upgrade -q mxnet==1.9.1\n",
        "!pip install -q autogluon==0.7.0\n",
        "\n",
        "#pythainlp\n",
        "#!pip install -q pythainlp==3.1.1\n",
        "!pip install pythainlp\n",
        "#transformers\n",
        "#!pip install -q transformers==4.27.3\n",
        "!pip install transformers\n",
        "exit() #reset runtime เพื่อให้ package ใช้งานได้อย่างถูกต้อง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UymtSzRrWHtN"
      },
      "source": [
        "## <font color='saddlebrown'>เริ่มทำ ML กัน! Image Classification with fastai and Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5-nzYnPvncL"
      },
      "source": [
        "ผลงาน [โมเดลจำแนกรูปอาหารไทย 48 ประเภท](https://www.facebook.com/groups/dsbkkgroup/posts/1733262050179128/) โดย [@GemmyTheGeek](https://github.com/GemmyTheGeek) ศิษย์เก่าโครงการ AI Builders (Class of 2021) เราจะเทรน ML model ด้วยข้อมูลรูปภาพอาหารไทยขนาด 224 x 224 pixels จำนวน 240 รูปต่อเมนูอาหาร (รวม 240 x 48 = 11,520 รูป) และทดสอบบน mini validation set ที่มีข้อมูล 5 รูปต่อเมนูอาหาร (รวม 5 x 48 = 240 รูป) ว่ามีความแม่นยำ (accuracy) แค่ไหน\n",
        "\n",
        "โมเดลที่ดีที่สุดของ [@GemmyTheGeek](https://github.com/GemmyTheGeek) มีความแม่นยำถึง 97% หลังผ่านการปรับแต่ง สามารถทดลองใช้ได้[ที่นี่](https://huggingface.co/spaces/cstorm125/foodydudy_for_lesson1/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xyOKtuB6Nsd"
      },
      "source": [
        "### FoodyDudy - ชุดข้อมูลอาหารไทย 48 ประเภท"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4hK2WHj6VCS"
      },
      "outputs": [],
      "source": [
        "#download รูปอาหารจาก FoodyDudy\n",
        "!git clone https://github.com/GemmyTheGeek/FoodyDudy.git\n",
        "!rm -r FoodyDudy/images/valid_mini\n",
        "!mkdir FoodyDudy/images/valid_mini\n",
        "\n",
        "#สร้าง valid mini เพื่อความเร็วในการทำ validation\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "valid_folders = glob.glob('FoodyDudy/images/valid/*')\n",
        "valid_fnames = []\n",
        "for folder in valid_folders:\n",
        "    valid_fnames += glob.glob(f'{folder}/*')[:5]\n",
        "for i in range(48):\n",
        "    os.mkdir(f'FoodyDudy/images/valid_mini/{str(i).zfill(2)}')\n",
        "for fname in valid_fnames:\n",
        "    shutil.copyfile(fname, f'FoodyDudy/images/valid_mini/{fname.split(\"/\")[-2]}/{fname.split(\"/\")[-1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eVr9PAP6XMp"
      },
      "source": [
        "<img src=https://github.com/GemmyTheGeek/FoodyDudy/raw/main/47thaifood.png width=\"500px\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lLQ1Knr6X1t"
      },
      "outputs": [],
      "source": [
        "#เปลี่ยน label จากตัวเลขเป็นชื่อภาษาอังกฤษ เพื่อความง่ายในการดูผล\n",
        "food_d = [\n",
        " 'green_curry', 'tepo_curry', 'liang_curry', 'taohoo_moosup', 'mara_yadsai',\n",
        " 'masaman', 'orange_curry', 'cashew_chicken', 'omelette', 'sunny_side_up',\n",
        " 'palo_egg', 'sil_egg', 'nun_banana', 'kua_gai', 'cabbage_fish_sauce',\n",
        " 'river_prawn', 'shrimp_ob_woonsen', 'kanom_krok', 'mango_sticky_rice', 'kao_kamoo',\n",
        " 'kao_klook_kapi', 'kaosoi', 'kao_pad', 'kao_pad_shrimp', 'chicken_rice',\n",
        " 'kao_mok_gai', 'tom_ka_gai', 'tom_yum_kung', 'tod_mun', 'poh_pia',\n",
        " 'pak_boong_fai_daeng', 'padthai', 'pad_krapao', 'pad_si_ew', 'pad_fakthong',\n",
        " 'eggplant_stirfry', 'pad_hoi_lai', 'foithong', 'panaeng', 'yum_tua_ploo',\n",
        " 'yum_woonsen', 'larb_moo', 'pumpkin_custard', 'sakoo_sai_moo', 'somtam',\n",
        " 'moopoing','satay', 'hor_mok'\n",
        "]\n",
        "\n",
        "for i in range(48):\n",
        "    os.rename(f\"FoodyDudy/images/train/{str(i).zfill(2)}\", f\"FoodyDudy/images/train/{str(i).zfill(2)}_{food_d[i]}\")\n",
        "for i in range(48):\n",
        "    os.rename(f\"FoodyDudy/images/valid_mini/{str(i).zfill(2)}\", f\"FoodyDudy/images/valid_mini/{str(i).zfill(2)}_{food_d[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgD6MFoQ6bLC"
      },
      "source": [
        "### เทรนโมเดลด้วย fastai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbZHTJr7oE_u"
      },
      "source": [
        "[fastai](https://docs.fast.ai/) เป็น package สำหรับเทรน deep learning model ที่สร้างต่อยอดบน [Pytorch](https://pytorch.org/) ทำให้เราสามารถเทรน deep learning model ที่มีประสิทธิภาพสูงได้ง่ายและรวดเร็ว"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nHzhYffWYgA"
      },
      "outputs": [],
      "source": [
        "from fastbook import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOV6-51VpNyq"
      },
      "source": [
        "เนื่องจากเรามีข้อมูลรูปภาพกว่า 1 หมื่นรูป เราจึงไม่สามารถโหลดรูปภาพทั้งหมดเข้าไปใน memory ได้ในครั้งเดียว เราจึงต้องอาศัย dataloader เพื่อทำการโหลดข้อมูลเข้าไปทีละ batch ในที่นี้คือ batch ละ 64 รูป\n",
        "\n",
        "`DataBlock` เป็น class สำหรับสร้าง dataloader เพื่อส่งรูปให้กับโมเดลของ fastai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vmfhigEWYgB"
      },
      "outputs": [],
      "source": [
        "dblock = DataBlock(\n",
        "    blocks=(ImageBlock, CategoryBlock), #x - image; y - single class\n",
        "    get_items=get_image_files, #get image\n",
        "    splitter=GrandparentSplitter(valid_name='valid_mini'), #use parent folder as train-valid split\n",
        "    get_y=parent_label, #use parent folder as label\n",
        "    batch_tfms=aug_transforms(size=224)\n",
        "    )\n",
        "dls = dblock.dataloaders('FoodyDudy/images/', bs=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqJxNXvsWYgB"
      },
      "outputs": [],
      "source": [
        "dls.train.show_batch(max_n=9,nrows=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LAKCkV4pq7Y"
      },
      "source": [
        "สร้าง trainer ชื่อ `learn` และโหลด pretrained weights ชื่อ `resnet34` เพื่อทำการเทรนโมเดล; pretrained weights คือ weights ที่ถูกเทรนเพื่อจำแนกรูปภาพสิ่งของ 1,000 ประเภทจากชุดข้อมูล [ImageNet](https://www.image-net.org/) กว่า 1.2 ล้านรูป เรานำโมเดลนี้มาปรับจูน (finetune) เพื่อใช้จำแนกรูปที่เราต้องการ เช่น รูปอาหารไทย 48 ชนิดแทน\n",
        "\n",
        "วิธีการนี้เรียกว่า [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning) กล่าวคือนำโมเดลที่ถูกเทรนเพื่อทำ task หนึ่ง (จำแนกรูปภาพสิ่งของทั่วไป, ทำนายคำต่อไปในประโยค ฯลฯ) มาปรับจูนเพื่อทำ task อื่นๆ (จำแนกรูปอาหารไทย, จำแนกชนิดประโยค ฯลฯ) เป็นเทคนิคที่นิยมใช้กันอย่างแพร่หลายใน deep learning โดยเฉพาะกับข้อมูลรูปภาพและข้อความ\n",
        "\n",
        "เราเทรนโมเดล (เฉพาะส่วนจำแนกรูปภาพอาหารไทย) ด้วยรูปทั้งหมด 1 ครั้ง (1 epoch) ด้วย learning rate = 0.002 เราจะวัดผลด้วย metric คือ accuracy (ความแม่นยำ; ถ้าทายถูกได้ 1 ทายผิดได้ 0 และหาค่าเฉลี่ย) ได้ความแม่นยำกว่า 75% ในเวลาไม่ถึง 3 นาทีซึ่งถือว่าไม่เลวเลยสำหรับการจำแนกรูปภาพ 48 ประเภท (ถ้าเดาสุ่มมีความแม่นยำ 1/48 = 2.08%)\n",
        "\n",
        "ศัพท์เทคนิคทั้งหมดนี้คุณอาจจะยังไม่ต้องเข้าใจทั้งหมดตอนนี้ แต่จะได้เรียนรู้ในบทเรียนต่อๆไป"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vyHd8IECWYgB"
      },
      "outputs": [],
      "source": [
        "learn = cnn_learner(dls, resnet34, metrics=accuracy)\n",
        "learn.fine_tune(epochs=0, freeze_epochs=1, base_lr=2e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNBF43zuqf7O"
      },
      "source": [
        "ดูผลการทำนายคร่าวๆใน validation set ว่าทำนายผิดจากอะไรเป็นอะไรบ้าง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEXtyBjWYgB"
      },
      "outputs": [],
      "source": [
        "learn.show_results() #true label - บน; prediction - ล่าง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZBuIl5juFaK"
      },
      "source": [
        "ดู confusion matrix ของผลการทำนายใจ validation set ทั้งหมด `Actual` คือ label ที่แท้จริง `Predicted` คือ label ที่โมเดลทำนาย"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9euKCfcWYgB"
      },
      "outputs": [],
      "source": [
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix(figsize=(10,10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBvCwKhtuU5A"
      },
      "source": [
        "Metric (ตัวชี้วัด) สำหรับ classification อื่นๆ เช่น [precision, recall, f1-score](https://en.wikipedia.org/wiki/Precision_and_recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejZZtADmWYgB"
      },
      "outputs": [],
      "source": [
        "interp.print_classification_report()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIWJHkhhYcmC"
      },
      "source": [
        "## <font color='saddlebrown'>Artificial Intelligence (AI), Machine Learning (ML), Deep Learning (DL) คืออะไร-เหมือน/ต่างกันอย่างไร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g093jBSHu9Bv"
      },
      "source": [
        "AI มีนิยามที่กว้างและหลากหลายเป็นอย่างยิ่ง หากอ้างอิงหนังสือ [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/) จะหมายถึงระบบที่คิดหรือทำอย่างมนุษย์หรืออย่างมีเหตุผล หรืออาจจะกล่าวได้ว่าเป็นระบบที่คิดและทำอย่างมี \"[สติปัญญา](https://en.wikipedia.org/wiki/Intelligence)\" ซึ่งเป็นคำที่มีนิยามกว้างและหลากหลายอีกเช่นกัน\n",
        "\n",
        "ในทางปฏิบัติของโครงการ AI Builders เราจะนิยาม AI ว่าเป็นระบบที่คิดและทำอย่างชาญฉลาด (โดยจงใจไม่ให้นิยามอันแน่ชัดกับความ \"ชาญฉลาด\" นี้) โดยอาศัย \"กฎ\" เช่น `ถ้าเกิด X แล้วควรจะทำ Y` กฎเหล่านี้จะถูกสร้างโดยมนุษย์กำหนดขึ้นเองหรือให้เครื่องจักรเรียนรู้ขึ้นเองจากข้อมูลก็ได้\n",
        "\n",
        "ML คือ AI ชนิดหนึ่งเกิดขึ้นในกรณีที่กฎถูกเรียนรู้ขึ้นมาจากข้อมูลในอดีตด้วยเทคนิคทางสถิต เช่น จากการสังเกตข้อมูลซื้อขายของลูกค้าในอดีต เราค้นพบกฎว่า `ถ้าเป็นวันเสาร์-อาทิตย์ ยอดขายจะเพิ่มขึ้น` เป็นต้น\n",
        "\n",
        "DL คือเทคนิค ML ที่ใช้ Neural Networks หลายชั้นเป็นสถาปัตยกรรมในการเรียนรู้กฎเหล่านี้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V44Aa8aPaZWH"
      },
      "source": [
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/ai_ml_dl.png width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CDMTKpsas92"
      },
      "source": [
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/ai_definitions.png width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jNLw860ZXJO"
      },
      "source": [
        "## <font color='saddlebrown'>ML ต่างจาก AI รูปแบบอื่นอย่างไร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNHcazcSyBe8"
      },
      "source": [
        "เราอาจจะแบ่ง AI คร่าวๆเป็น 2 ประเภทคือ\n",
        "\n",
        "1. **Rule-based Systems** ที่กฎ (Rules) ถูกกำหนดขึ้นโดยมนุษย์ เมื่อเราใส่ Input และ Rules ที่เรากำหนดเข้าไปคำนวณ (Computation) ก็จะได้ Output ออกมา\n",
        "\n",
        "2. **Machine Learning Systems** ที่กฎถูกเรียนรู้ขึ้นมาจาก Input และ Output ในอดีต เช่น พฤติกรรมลูกค้าในอดีต, รูปภาพหรือข้อความที่ถูก label ด้วยมนุษย์มาก่อน, ราคาทรัพย์สินในอดีต เป็นต้น เพื่อให้ได้ Rules ขึ้นมา โดยส่วนใหญ่เรียก Rules นี้ว่า ML Models หลังจากนั้นจึงทำ Rules ไปใช้งานกับข้อมูลชุดใหม่ในรูปแบบเดียวกับ Rule-based Systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CAohTi8a2gB"
      },
      "source": [
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/rules_vs_ml.png width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhX-6hf_Yi0x"
      },
      "source": [
        "## <font color='saddlebrown'>ML Model เรียนรู้ได้อย่างไร"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4KWxOSXz16w"
      },
      "source": [
        "ตัวอย่าง `Training Loop` การจำแนกรูป `ส้มตำ` หรือ `ไม่ส้มตำ`\n",
        "\n",
        "1. เตรียมข้อมูล `Inputs` (ตัวอย่าง; รูปภาพ) และ `Labels` (`ส้มตำ` หรือ `ไม่ส้มตำ`) ใส่เข้าไปใน `Dataloader`; `Dataloader` ทำหน้าที่ส่งข้อมูลให้ทีละ batch (จำนวนตัวอย่างต่อ batch เรียกว่า batch size)\n",
        "2. ส่งข้อมูล Inputs ใน batch นั้นไปคำนวนด้วย `Weights` เพื่อให้ได้ `Predictions`; `Weights` เปรียบเสมือนตัวเลขค่าสัมประสิทธิ์ในสมการ เช่น ในสมการ `y = 2.5x^2 - 0.5x + 0.2` ค่า `Weights` คือ 2.5, -0.5  และ 0.2 (`x` คือ `Inputs` และ `y` คือ `Predictions`)\n",
        "3. คำนวณ `Loss` ของ batch ด้วย `Loss Function`; `Loss Function` มีหน้าที่หา \"ความแตกต่าง\" ระหว่าง `Predictions` (คำทำนายจากโมเดลว่ามีโอกาสเป็น `ส้มตำ` แค่ไหน) และ `Labels` (ความเป็นจริงว่าเป็น `ส้มตำ` หรือ `ไม่ส้มตำ`) ค่า `Loss` คือผลลัพธ์ ยิ่ง `Loss` สูงก็ยิ่งแปลว่าคำทำนายของเราผิดมาก-เยอะ\n",
        "4. หลังจากได้ `Loss` ของแต่ละ batch แล้วเราก็คำนวณ `Gradients` ด้วยวิธี Backpropagation และนำ `Gradients` ไปให้ `Optimizer` ทำการ update `Weights`; การอัพเดทในที่นี้กล่าวง่ายๆคือการเปลี่ยนตัวเลขในสมการ เช่น จาก `y = 0.1x^2 - 0.1x + 0.1 เป็น `y` = 2.5x^2 - 0.5x + 0.2` เป็นต้น โดยการทำเช่นนี้โดยทั่วไปจะทำให้โมเดลมี `Weights` ที่ทำนายได้แม่นยำมากขึ้นและส่งผลให้ `Loss` ลดลงเรื่อยๆ\n",
        "\n",
        "ขั้นตอนการกระทำ 1.-4. สำหรับข้อมูล 1 batch เรียกว่า 1 iteration; หากทำจนครบทุก batch ของข้อมูลที่มี 1 ครั้งเรียกว่า 1 epoch เช่น หากเรามีข้อมูล 100 รูป, batch size = 5 รูป, เราจะมีทั้งหมด 20 batches, 1 epoch ของเราจะประกอบด้วย 20 iterations เครื่องมือการเทรน ML สมัยใหม่ส่วนใหญ่จะมี class สำหรับช่วยทำให้ `Training Loop` สะดวกขึ้นเรียกว่า `Trainer` และ class สำหรับเก็บข้อมูลระหว่างเทรนเรียกว่า `Logger` เพิ่มมาอีกด้วย\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OtDEaMwciwg"
      },
      "source": [
        "คุณเห็นความแตกต่างอะไรบ้างใน `Training Loop` ด้านล่าง?\n",
        "\n",
        "**ก่อนเทรน**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lti7t0GJcpAg"
      },
      "source": [
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/training_loop50.png width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cCy-sKnclMw"
      },
      "source": [
        "**หลังเทรน**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGUNDougca8N"
      },
      "source": [
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/training_loop70.png width=\"700px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA7AEFPk5dov"
      },
      "source": [
        "## <font color='saddlebrown'>Metric and Splits - โมเดลทำได้ดีแค่ไหน-แค่ไหนเรียกดี"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEENs0fw5hP8"
      },
      "source": [
        "ค่า `Loss` สามารถบอกได้ว่า `Predictions` \"แตกต่าง\" จาก `Labels` มากแค่ไหน แต่ในการใช้งานจริงคุณไม่สามารถไปบอกคนอื่นว่า \"โมเดลจำแนกสิ่งของ 10 ชนิดของผมมี `Loss` อยู่ที่ 0.234\" แล้วให้เขาเข้าใจว่ามันดีหรือไม่ดี เราจึงต้องมี `Metric` (ตัวชี้วัด) ที่เหมาะสมกับงาน (ภาษาทางเทคนิคนิยมเรียกว่า `task`) ต่างๆ เช่น หากเป็นการจำแนกรูปภาพสิ่งของ (`image classification task`) เราอาจจะใช้ `Metric` เป็น accuracy, [precision, recall, หรือ f1-score](https://en.wikipedia.org/wiki/Precision_and_recall) หากเป็นการทำนายตัวเลข (`regression task`) เราอาจจะใช้ [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error), [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error), หรือ [mean absolute percentage error](https://en.wikipedia.org/wiki/Mean_absolute_percentage_error)\n",
        "\n",
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/train_valid_test_splits.png width=\"500px\">\n",
        "\n",
        "โดยทั่วไปแล้ว เราจะแบ่งชุดข้อมูลเป็น 3 splits และคำนวณค่า metric บน `validation` และ `test` sets\n",
        "\n",
        "1. `train` - ชุดข้อมูลที่ป้อนให้โมเดลเพื่อทำการเรียนรู้ตาม `Training Loop` (\"แบบฝึกหัด\")\n",
        "2. `validation` - ชุดข้อมูลสำหรับคำนวณ metric เพื่อเลือกโมเดลที่ดีที่สุด เพราะในความเป็นจริงเราอาจจะเทรนโมเดลหลากหลายสถาปัตยกรรมหรือ hyperparameters; metric ที่ถูกคำนวณบน `validation` จึงใช้สำหรับเปรียบเทียบโมเดลที่ดีที่สุด (\"ข้อสอบ pre-test จากโรงเรียนกวดวิชา\")\n",
        "3. `test` - ชุดข้อมูลสำหรับคำนวณ metric ของโมเดลที่เราเลือกแล้วว่าดีที่สุด เปรียบเสมือนตัวชี้วัดว่าโมเดลของเราจะทำได้ดีแค่ไหนในโลกแห่งความเป็นจริง; ชุดข้อมูลนี้จะถูกรันเพียงครั้งเดียวเท่านั้นในการทำโครงงาน ด้วยโมเดลที่ดีที่สุดที่เราเลือกมาจาก `validation` (\"ข้อสอบจริง\")\n",
        "\n",
        "เหตุผลที่เราแบ่งชุดข้อมูล `train` กับ `validation` ออกจากกันเนื่องจากหากโมเดลได้รับการเทรนด้วยตัวอย่างใดตัวอย่างหนึ่งแล้ว มีความเป็นไปได้สูงที่โมเดลจะ \"จดจำคำตอบ\" ของตัวอย่างนั้นๆได้ เปรียบเสมือนหากเรานำข้อสอบปลายภาคมาให้นักเรียนทำเป็นแบบฝึกหัด นักเรียนย่อมมีโอกาสทำได้ดีเกือบเต็มแทบทุกคน แต่ไม่ใช่เพราะนักเรียนได้เรียนรู้สิ่งที่เราต้องการ แต่แค่เพราะจำคำตอบได้จากแบบฝึกหัดนั่นเอง\n",
        "\n",
        "เหตุผลที่เราแบ่งชุดข้อมูล `validation` กับ `test` ออกจากกันเนื่องจาก หากเราใช้ข้อมูลชุดเดียวกันในการทดสอบโมเดลหลายรูปแบบ เรามีโอกาสที่จะจงใจปรับจูนโมเดลให้เหมาะกับ \"ข้อสอบจริง\" ได้โดยตั้งใจหรือไม่ตั้งใจ เราจึงปรับจูนโมเดลของเราด้วย `validation` แล้วจึงนำโมเดลที่ดีที่สุดไป \"สอบจริง\" ด้วย `test`\n",
        "\n",
        "เพราะฉะนั้นเวลาเราแบ่ง `train`, `validation` และ `test` splits เราต้องทำให้มั่นใจว่าไม่มีตัวอย่างเดียวกันหลุดไปใน set อื่น เช่น ในการทำ face recognition ไม่ควรมีรูปคนๆเดียวกันในมากกว่า 1 set, หากทำ time series forecasting เรียงข้อมูลจากเก่าสุดไปใหม่สุดใน `train`, `validation` และ `test` เพื่อไม่ให้โมเดลเห็นข้อมูลจากอนาคต, หากทำ product recommendation ไม่ควรมีข้อมูลของลูกค้าคนเดียวกันในมากกว่า 1 set เพราะจะทำให้เราเดาว่าเขาเป็นลูกค้าประเภทไหนได้ด้วยข้อมูลที่ไม่ควรเห็น ฯลฯ ทั้งนี้ขึ้นอยู่กับวิจารณญาณของผู้สร้างโมเดล\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN8vbB5AO-WV"
      },
      "source": [
        "## <font color='saddlebrown'>Image Classification with Pytorch and Resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eI1bvy2Dtfd0"
      },
      "source": [
        "เรามาลองจำแนกรูปอาหารไทย 48 ประเภทกันอีกครั้ง โดยครั้งนี้เราจะค่อยๆสร้างส่วนประกอบต่างๆขึ้นเองด้วย [Pytorch](https://pytorch.org/) และ [Pytorch Ignite](https://pytorch.org/ignite/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6NLST65QXlH"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as T\n",
        "from torchvision import models\n",
        "\n",
        "from ignite.engine import Engine, Events, create_supervised_evaluator, create_supervised_trainer\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "\n",
        "try:\n",
        "    from tensorboardX import SummaryWriter\n",
        "except ImportError:\n",
        "    try:\n",
        "        from torch.utils.tensorboard import SummaryWriter\n",
        "    except ImportError:\n",
        "        raise RuntimeError(\n",
        "            \"This module requires either tensorboardX or torch >= 1.2.0. \"\n",
        "            \"You may install tensorboardX with command: \\n pip install tensorboardX \\n\"\n",
        "            \"or upgrade PyTorch using your package manager of choice (pip or conda).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6IhXDpkT8xN"
      },
      "source": [
        "### Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YIFewgxsEjI"
      },
      "outputs": [],
      "source": [
        "probs = torch.tensor([[0.4, 0.6],\n",
        "                      [0.1, 0.9],\n",
        "                      [0.9, 0.1]], dtype=torch.float)\n",
        "preds = probs.argmax(1)\n",
        "targets = torch.tensor([0,1,0])\n",
        "\n",
        "print(probs)\n",
        "print(preds)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rv-Nav4iUBpt"
      },
      "outputs": [],
      "source": [
        "#accuracy\n",
        "(preds==targets).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CTEsZjsTwid"
      },
      "source": [
        "### Datasets and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNpdA_6cvFPm"
      },
      "outputs": [],
      "source": [
        "train_dir = 'FoodyDudy/images/train'\n",
        "valid_dir = 'FoodyDudy/images/valid_mini'\n",
        "batch_size = 256\n",
        "\n",
        "#image augmentations; fastai ทำให้เราอัตโนมัติ แต่เราสามารถกำหนดเองได้ด้วยว่าเราอยากได้อะไรบ้าง\n",
        "train_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.TrivialAugmentWide(),\n",
        "    T.ToTensor(),\n",
        "    # T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "valid_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    # T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "#dataset\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "valid_ds = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
        "\n",
        "#dataloader\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=0, shuffle=False)\n",
        "\n",
        "classes = train_ds.classes\n",
        "n_classes = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWjoMHRNvqeS"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "#helper functions from https://pytorch.org/vision/master/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=True, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols,\n",
        "                            squeeze=False,)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [orig_img] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "test_images = glob.glob('FoodyDudy/images/test/00/*')\n",
        "i = np.random.randint(0,len(test_images))\n",
        "orig_img = Image.open(test_images[i])\n",
        "\n",
        "augmenter = T.TrivialAugmentWide()\n",
        "imgs = [augmenter(orig_img) for _ in range(2)]\n",
        "plot(imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nSl17Zn0cmt"
      },
      "outputs": [],
      "source": [
        "#helper functions adapted from https://pytorch.org/vision/master/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot_row(imgs, img_per_row = 3):\n",
        "    num_cols = img_per_row\n",
        "    num_rows = len(imgs) // num_cols\n",
        "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        ax = axs[row_idx // num_cols, row_idx % num_cols]\n",
        "        ax.imshow(row.permute(1,2,0))\n",
        "        ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "    plt.tight_layout()\n",
        "\n",
        "sample_imgs = next(iter(train_dl))\n",
        "plot_row(sample_imgs[0][:9])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN4FfSQGTI5P"
      },
      "source": [
        "### Architecture and Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzJYuAw0uFAK"
      },
      "source": [
        "เราเลือกใช้ Architecture ชื่อ [resnet](https://pytorch.org/vision/stable/models.html) รูปแบบที่มี 50 layers (`resnet50`) ที่ถูก pretrain (เทรนมาก่อนหน้า) ด้วยรูปสิ่งของทั่วไปกว่า 1 ล้านรูป ([ImageNet](https://www.image-net.org/))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P14QADF2WLsk"
      },
      "outputs": [],
      "source": [
        "backbone = models.resnet50(pretrained=True)\n",
        "backbone"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7YHBxsWBlt"
      },
      "source": [
        "### Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt_zzrs7un6O"
      },
      "source": [
        "เราเลือกใช้ Loss Function ที่เรียกว่า [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2zZSOzas4dx"
      },
      "outputs": [],
      "source": [
        "#ลองปรับ probs และ targets ดูว่าทำให้ loss เปลี่ยนไปอย่างไรบ้าง\n",
        "probs = torch.tensor([[0.4, 0.6],\n",
        "                      [0.1, 0.9],\n",
        "                      [0.9, 0.1]], dtype=torch.float)\n",
        "preds = probs.argmax(1)\n",
        "targets = torch.tensor([0,1,0])\n",
        "\n",
        "print(probs)\n",
        "print(preds)\n",
        "print(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qf-i57YWEIc"
      },
      "outputs": [],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "loss_function(input = probs,\n",
        "              target = targets).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqQowgx_rZWE"
      },
      "outputs": [],
      "source": [
        "#accuracy\n",
        "(preds==targets).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54WxTbPZWCka"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfB6AN4uuxnC"
      },
      "source": [
        "เราใช้ [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) เป็น Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xenxHSmuWEgZ"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(backbone.parameters(), lr=3e-4)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caEpy-SVWVn-"
      },
      "source": [
        "### Architecture as Class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0G3NPW4u3bt"
      },
      "source": [
        "เราเปลี่ยน layer สุดท้ายของ resnet50 จากการทำนายสิ่งของ 1,000 ประเภทเป็นการทำนายอาหาร 48 ประเภท"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifciV0ckRsiU"
      },
      "outputs": [],
      "source": [
        "class FoodResNet(nn.Module):\n",
        "    def __init__(self, n_classes=48):\n",
        "        super(FoodResNet, self).__init__()\n",
        "        # ใช้สถาปัตยกรรม resnet50; เปลี่ยน layer สุดท้ายเป็นการทำนายอาหาร 48 classes\n",
        "        self.backbone = models.resnet50(pretrained=True)\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        #เพิ่งแค่ตรงนี้\n",
        "        self.backbone.fc = torch.nn.Linear(self.backbone.fc.in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        preds = self.backbone(x)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdRU9WVYTzq8"
      },
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p21bmr0IvFfN"
      },
      "source": [
        "นำทุกสิ่งเข้ามารวมกันแล้วเริ่มการเทรน!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pbVX0orrFu"
      },
      "source": [
        "#### Putting It All Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzBt1YXszGEJ"
      },
      "outputs": [],
      "source": [
        "train_dir = 'FoodyDudy/images/train'\n",
        "valid_dir = 'FoodyDudy/images/valid_mini'\n",
        "batch_size = 512\n",
        "\n",
        "#image augmentations\n",
        "train_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.TrivialAugmentWide(), #image augmentation จากงาน https://arxiv.org/abs/2103.10158\n",
        "    T.ToTensor(),\n",
        "    # T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "valid_transform = T.Compose([\n",
        "    T.Resize(224),\n",
        "    T.ToTensor(),\n",
        "    # T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "#dataset\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "valid_ds = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
        "\n",
        "#dataloader\n",
        "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=0, shuffle=False)\n",
        "\n",
        "classes = train_ds.classes\n",
        "n_classes = len(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPYCDci0R-rx"
      },
      "outputs": [],
      "source": [
        "#ใส่ model และ input ใน gpu (ถ้ามี) ไม่ก็ cpu\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "#architecture\n",
        "model = FoodResNet(n_classes=len(train_ds.classes)).to(device)\n",
        "\n",
        "#loss\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "#optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "#trainer\n",
        "trainer = create_supervised_trainer(model, optimizer, loss_function, device=device)\n",
        "\n",
        "#metric\n",
        "val_metrics = {\"accuracy\": Accuracy(), \"ce_loss\": Loss(loss_function)}\n",
        "evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMoA8ch_ruDg"
      },
      "source": [
        "#### Logger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxe63-xZvJNe"
      },
      "source": [
        "Logger สำหรับบันทึก Metric ระหว่างการเทรน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F90I1hdNuVV3"
      },
      "outputs": [],
      "source": [
        "#logger\n",
        "writer = SummaryWriter(log_dir='logs')\n",
        "train_log_interval = 1 #1 iteration = 1 batch = example จำนวนเท่า batch size\n",
        "eval_log_interval = len(train_dl) #1 epoch = จำนวน example ทั้งหมด / batch size = วนครบทุก example 1 รอบ\n",
        "\n",
        "pbar = tqdm(initial=0,\n",
        "            leave=False,\n",
        "            total=len(train_dl),\n",
        "            desc=f\"epoch {0} - loss: {0:.4f} - lr: {0:.4f}\")\n",
        "\n",
        "#ทุก X iteration, บันทึก training loss\n",
        "@trainer.on(Events.ITERATION_COMPLETED(every=train_log_interval))\n",
        "def log_training_loss(engine):\n",
        "    lr = optimizer.param_groups[0][\"lr\"]\n",
        "    pbar.desc = f\"epoch {engine.state.epoch} - train loss: {engine.state.output:.4f} - lr: {lr:.4f}\"\n",
        "    pbar.update(train_log_interval)\n",
        "    writer.add_scalar(\"training/loss\", engine.state.output, engine.state.iteration)\n",
        "\n",
        "#ทุก epoch, บันทึก training loss, accuracy\n",
        "@trainer.on(Events.ITERATION_COMPLETED(every=eval_log_interval))\n",
        "def log_training_results(engine):\n",
        "    evaluator.run(train_dl)\n",
        "    metrics = evaluator.state.metrics\n",
        "    avg_accuracy = metrics[\"accuracy\"]\n",
        "    avg_loss = metrics[\"ce_loss\"]\n",
        "    tqdm.write(\n",
        "        f\"train results - epoch: {engine.state.epoch} avg accuracy: {avg_accuracy:.2f} avg loss: {avg_loss:.2f}\"\n",
        "    )\n",
        "    writer.add_scalar(\"training/avg_loss\", avg_loss, engine.state.iteration)\n",
        "    writer.add_scalar(\"training/avg_accuracy\", avg_accuracy, engine.state.iteration)\n",
        "\n",
        "#ทุก epoch, บันทึก validation loss, accuracy\n",
        "@trainer.on(Events.ITERATION_COMPLETED(every=eval_log_interval))\n",
        "def log_validation_results(engine):\n",
        "    evaluator.run(valid_dl)\n",
        "    metrics = evaluator.state.metrics\n",
        "    avg_accuracy = metrics[\"accuracy\"]\n",
        "    avg_loss = metrics[\"ce_loss\"]\n",
        "    tqdm.write(\n",
        "        f\"valid results - epoch: {engine.state.epoch} avg accuracy: {avg_accuracy:.2f} avg loss: {avg_loss:.2f}\"\n",
        "    )\n",
        "    writer.add_scalar(\"valdation/avg_loss\", avg_loss, engine.state.iteration)\n",
        "    writer.add_scalar(\"valdation/avg_accuracy\", avg_accuracy, engine.state.iteration)\n",
        "    pbar.n = pbar.last_print_n = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgkYCzyEr2t_"
      },
      "source": [
        "#### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JZYnbYMwM0z"
      },
      "outputs": [],
      "source": [
        "trainer.run(train_dl, max_epochs=3)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pnJFYYdr_Hc"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ct1siNNZ41h"
      },
      "source": [
        "ดู Log ของการเทรนด้วย `tensorboard`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqa6yO-UwnDt"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir logs/ --host 0.0.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xupbBmXzPB9U"
      },
      "source": [
        "## <font color='saddlebrown'>Text Classification with HuggingFace and WangchanBERTa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAcMG3wCcgaK"
      },
      "source": [
        "ใช้โมเดลที่ดีที่สุด ทำ NLP task ที่ง่ายที่สุดคือการแยกข้อความ social media ว่าเป็น positive, neutral, negative หรือ question ด้วยชุดข้อมูล [wisesight_sentiment](https://github.com/PyThaiNLP/wisesight-sentiment) ผล benchmark ที่ดีที่สุดของชุดข้อมูลนี้ทำโดย [Wangchanberta](https://medium.com/airesearch-in-th/wangchanberta-%E0%B9%82%E0%B8%A1%E0%B9%80%E0%B8%94%E0%B8%A5%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%A1%E0%B8%A7%E0%B8%A5%E0%B8%9C%E0%B8%A5%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%E0%B9%84%E0%B8%97%E0%B8%A2%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B9%83%E0%B8%AB%E0%B8%8D%E0%B9%88%E0%B9%81%E0%B8%A5%E0%B8%B0%E0%B8%81%E0%B9%89%E0%B8%B2%E0%B8%A7%E0%B8%AB%E0%B8%99%E0%B9%89%E0%B8%B2%E0%B8%97%E0%B8%B5%E0%B9%88%E0%B8%AA%E0%B8%B8%E0%B8%94%E0%B9%83%E0%B8%99%E0%B8%82%E0%B8%93%E0%B8%B0%E0%B8%99%E0%B8%B5%E0%B9%89-d920c27cd433) ที่ micro-averaged F1 ที่ 76.19\n",
        "\n",
        "เพื่อประหยัดเวลาในตัวอย่างนี้เราจะเทรนด้วย training set เพียงแค่ 5,000 ตัวอย่างจากกว่า 20,000 ข้อความ\n",
        "\n",
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/wangchanberta_results.png width=\"700px\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Preprocessing for thai2transformers\n",
        "\"\"\"\n",
        "from typing import Collection, Callable\n",
        "from functools import partial\n",
        "import re\n",
        "import html\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "_TK_UNK, _TK_REP, _TK_WREP, _TK_URL, _TK_END = \"<unk> <rep> <wrep> <url> </s>\".split()\n",
        "\n",
        "SPACE_SPECIAL_TOKEN = \"<_>\"\n",
        "\n",
        "# str->str rules\n",
        "def fix_html(text: str) -> str:\n",
        "    \"\"\"\n",
        "        List of replacements from html strings in `test`. (code from `fastai`)\n",
        "        :param str text: text to replace html string\n",
        "        :return: text where html strings are replaced\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> fix_html(\"Anbsp;amp;nbsp;B @.@ \")\n",
        "            A & B.\n",
        "    \"\"\"\n",
        "    re1 = re.compile(r\"  +\")\n",
        "    text = (\n",
        "        text.replace(\"#39;\", \"'\")\n",
        "        .replace(\"amp;\", \"&\")\n",
        "        .replace(\"#146;\", \"'\")\n",
        "        .replace(\"nbsp;\", \" \")\n",
        "        .replace(\"#36;\", \"$\")\n",
        "        .replace(\"\\\\n\", \"\\n\")\n",
        "        .replace(\"quot;\", \"'\")\n",
        "        .replace(\"<br />\", \"\\n\")\n",
        "        .replace('\\\\\"', '\"')\n",
        "        .replace(\" @.@ \", \".\")\n",
        "        .replace(\" @-@ \", \"-\")\n",
        "        .replace(\" @,@ \", \",\")\n",
        "        .replace(\"\\\\\", \" \\\\ \")\n",
        "    )\n",
        "    return re1.sub(\" \", html.unescape(text))\n",
        "\n",
        "\n",
        "def replace_url(text: str) -> str:\n",
        "    \"\"\"\n",
        "        Replace url in `text` with TK_URL (https://stackoverflow.com/a/6041965)\n",
        "        :param str text: text to replace url\n",
        "        :return: text where urls  are replaced\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> replace_url(\"go to https://github.com\")\n",
        "            go to <url>\n",
        "    \"\"\"\n",
        "    URL_PATTERN = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
        "    return re.sub(URL_PATTERN, _TK_URL, text)\n",
        "\n",
        "\n",
        "def rm_brackets(text: str) -> str:\n",
        "    \"\"\"\n",
        "        Remove all empty brackets and artifacts within brackets from `text`.\n",
        "        :param str text: text to remove useless brackets\n",
        "        :return: text where all useless brackets are removed\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> rm_brackets(\"hey() whats[;] up{*&} man(hey)\")\n",
        "            hey whats up man(hey)\n",
        "    \"\"\"\n",
        "    # remove empty brackets\n",
        "    new_line = re.sub(r\"\\(\\)\", \"\", text)\n",
        "    new_line = re.sub(r\"\\{\\}\", \"\", new_line)\n",
        "    new_line = re.sub(r\"\\[\\]\", \"\", new_line)\n",
        "    # brakets with only punctuations\n",
        "    new_line = re.sub(r\"\\([^a-zA-Z0-9ก-๙]+\\)\", \"\", new_line)\n",
        "    new_line = re.sub(r\"\\{[^a-zA-Z0-9ก-๙]+\\}\", \"\", new_line)\n",
        "    new_line = re.sub(r\"\\[[^a-zA-Z0-9ก-๙]+\\]\", \"\", new_line)\n",
        "    # artifiacts after (\n",
        "    new_line = re.sub(r\"(?<=\\()[^a-zA-Z0-9ก-๙]+(?=[a-zA-Z0-9ก-๙])\", \"\", new_line)\n",
        "    new_line = re.sub(r\"(?<=\\{)[^a-zA-Z0-9ก-๙]+(?=[a-zA-Z0-9ก-๙])\", \"\", new_line)\n",
        "    new_line = re.sub(r\"(?<=\\[)[^a-zA-Z0-9ก-๙]+(?=[a-zA-Z0-9ก-๙])\", \"\", new_line)\n",
        "    # artifacts before )\n",
        "    new_line = re.sub(r\"(?<=[a-zA-Z0-9ก-๙])[^a-zA-Z0-9ก-๙]+(?=\\))\", \"\", new_line)\n",
        "    new_line = re.sub(r\"(?<=[a-zA-Z0-9ก-๙])[^a-zA-Z0-9ก-๙]+(?=\\})\", \"\", new_line)\n",
        "    new_line = re.sub(r\"(?<=[a-zA-Z0-9ก-๙])[^a-zA-Z0-9ก-๙]+(?=\\])\", \"\", new_line)\n",
        "    return new_line\n",
        "\n",
        "\n",
        "def replace_newlines(text: str) -> str:\n",
        "    \"\"\"\n",
        "        Replace newlines in `text` with spaces.\n",
        "        :param str text: text to replace all newlines with spaces\n",
        "        :return: text where all newlines are replaced with spaces\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> rm_useless_spaces(\"hey whats\\n\\nup\")\n",
        "            hey whats  up\n",
        "    \"\"\"\n",
        "\n",
        "    return re.sub(r\"[\\n]\", \" \", text.strip())\n",
        "\n",
        "\n",
        "def rm_useless_spaces(text: str) -> str:\n",
        "    \"\"\"\n",
        "        Remove multiple spaces in `text`. (code from `fastai`)\n",
        "        :param str text: text to replace useless spaces\n",
        "        :return: text where all spaces are reduced to one\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> rm_useless_spaces(\"oh         no\")\n",
        "            oh no\n",
        "    \"\"\"\n",
        "    return re.sub(\" {2,}\", \" \", text)\n",
        "\n",
        "def replace_spaces(text: str, space_token: str = SPACE_SPECIAL_TOKEN) -> str:\n",
        "    \"\"\"\n",
        "        Replace spaces with _\n",
        "        :param str text: text to replace spaces\n",
        "        :return: text where all spaces replaced with _\n",
        "        :rtype: str\n",
        "        :Example:\n",
        "            >>> replace_spaces(\"oh no\")\n",
        "            oh_no\n",
        "    \"\"\"\n",
        "    return re.sub(\" \", space_token, text)\n",
        "\n",
        "\n",
        "def replace_rep_after(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Replace repetitions at the character level in `text`\n",
        "    :param str text: input text to replace character repetition\n",
        "    :return: text with repetitive tokens removed.\n",
        "    :rtype: str\n",
        "    :Example:\n",
        "        >>> text = \"กาาาาาาา\"\n",
        "        >>> replace_rep_after(text)\n",
        "        'กา'\n",
        "    \"\"\"\n",
        "\n",
        "    def _replace_rep(m):\n",
        "        c, cc = m.groups()\n",
        "        return f\"{c}\"\n",
        "\n",
        "    re_rep = re.compile(r\"(\\S)(\\1{3,})\")\n",
        "    return re_rep.sub(_replace_rep, text)\n",
        "\n",
        "\n",
        "def replace_wrep_post(toks: Collection[str]) -> Collection[str]:\n",
        "    \"\"\"\n",
        "    Replace reptitive words post tokenization;\n",
        "    fastai `replace_wrep` does not work well with Thai.\n",
        "    :param Collection[str] toks: list of tokens\n",
        "    :return: list of tokens where repetitive words are removed.\n",
        "    :rtype: Collection[str]\n",
        "    :Example:\n",
        "        >>> toks = [\"กา\", \"น้ำ\", \"น้ำ\", \"น้ำ\", \"น้ำ\"]\n",
        "        >>> replace_wrep_post(toks)\n",
        "        ['กา', 'น้ำ']\n",
        "    \"\"\"\n",
        "    previous_word = None\n",
        "    rep_count = 0\n",
        "    res = []\n",
        "    for current_word in toks + [_TK_END]:\n",
        "        if current_word == previous_word:\n",
        "            rep_count += 1\n",
        "        elif (current_word != previous_word) & (rep_count > 0):\n",
        "            res += [previous_word]\n",
        "            rep_count = 0\n",
        "        else:\n",
        "            res.append(previous_word)\n",
        "        previous_word = current_word\n",
        "    return res[1:]\n",
        "\n",
        "\n",
        "# combine them together\n",
        "def process_transformers(\n",
        "    text: str,\n",
        "    pre_rules: Collection[Callable] = [\n",
        "        fix_html,\n",
        "        rm_brackets,\n",
        "        replace_newlines,\n",
        "        rm_useless_spaces,\n",
        "        replace_spaces,\n",
        "        replace_rep_after,\n",
        "    ],\n",
        "    tok_func: Callable = word_tokenize,\n",
        "    post_rules: Collection[Callable] = [replace_wrep_post],\n",
        ") -> str:\n",
        "    text = text.lower()\n",
        "    for rule in pre_rules:\n",
        "        text = rule(text)\n",
        "    toks = tok_func(text)\n",
        "    for rule in post_rules:\n",
        "        toks = rule(toks)\n",
        "    return \"\".join(toks)\n",
        "\n",
        "\"\"\"\n",
        "Metric for thai2transformers\n",
        "\"\"\"\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, classification_report\n",
        "def classification_metrics(pred, pred_labs=False):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions if pred_labs else pred.predictions.argmax(-1)\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n",
        "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(labels, preds, average=\"micro\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1_micro': f1_micro,\n",
        "        'precision_micro': precision_micro,\n",
        "        'recall_micro': recall_micro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'nb_samples': len(labels)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "AXatQA6GVhxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGgYYOCuPmZp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, load_metric, DatasetDict\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0veTJFkK_tMp"
      },
      "outputs": [],
      "source": [
        "#load dataset\n",
        "train_ds, valid_ds, test_ds = load_dataset('wisesight_sentiment', split=['train[:5000]','validation','test'])\n",
        "dataset = DatasetDict({'train': train_ds, 'validation': valid_ds, 'test': test_ds})\n",
        "dataset = dataset.map(lambda examples: {'labels': examples['category']}, batched=True)\n",
        "num_labels = len(set(dataset['train']['labels']))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjK0LVpEAzwX"
      },
      "outputs": [],
      "source": [
        "#ตัวอย่างข้อความ\n",
        "#0 positive; 1 neutral; 2 negative; 3 question\n",
        "pd.DataFrame(dataset['train'].shuffle()[:10])[['labels','texts']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJidqmwzFQg2"
      },
      "outputs": [],
      "source": [
        "#ทำความสะอาดข้อความ\n",
        "def clean_function(examples):\n",
        "    examples['texts'] = process_transformers(examples['texts'])\n",
        "    return examples\n",
        "\n",
        "cleaned_dataset = dataset.map(clean_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMz8C_BeFec_"
      },
      "outputs": [],
      "source": [
        "#ตัวอย่างข้อความที่ทำความสะอาดแล้ว\n",
        "pd.DataFrame(cleaned_dataset['train'].shuffle()[:10])[['labels','texts']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jehtEUJoBUfZ"
      },
      "outputs": [],
      "source": [
        "#tokenize ข้อความ; เปลี่ยนจาก texts เป็น index\n",
        "tokenizer = AutoTokenizer.from_pretrained('airesearch/wangchanberta-base-att-spm-uncased')\n",
        "tokenizer(cleaned_dataset['train'][0]['texts'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9DHRM6r8T13"
      },
      "outputs": [],
      "source": [
        "def encode_function(examples):\n",
        "    return tokenizer(examples['texts'], max_length=416, truncation=True)\n",
        "encoded_dataset = dataset.map(encode_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xi75ch1i8z6Z"
      },
      "outputs": [],
      "source": [
        "#โหลดโมเดล Wangchanberta ที่ถูก pretrained มาใช้สำหรับ sequence classification\n",
        "num_labels = len(set(encoded_dataset['train']['labels']))\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    'airesearch/wangchanberta-base-att-spm-uncased',\n",
        "    num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXCdts0R9CqN"
      },
      "outputs": [],
      "source": [
        "#setup trainer\n",
        "train_args = TrainingArguments(\n",
        "    output_dir = 'wisesight_sentiment_wangchanberta',\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps = int(len(encoded_dataset['train']) * 1 // 8 * 0.1),\n",
        "    weight_decay=1e-2,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model='f1_micro',\n",
        "    seed = 125\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    train_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=classification_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw-hsKvs9iqV"
      },
      "outputs": [],
      "source": [
        "#เริ่มเทรน\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W75Yg1YN93rn"
      },
      "outputs": [],
      "source": [
        "#ผลลัพธ์\n",
        "preds  = trainer.predict(encoded_dataset['test'])\n",
        "pd.DataFrame.from_dict(preds[2],orient='index')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSpwTC6M8mIg"
      },
      "source": [
        "## <font color='saddlebrown'>Text Classification with scikit-learn and Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fn-W7xbck8z"
      },
      "source": [
        "ใช้โมเดลง่ายๆทำ NLP task ง่ายๆ แล้วลองดูว่าได้ผลต่างกันแค่ไหน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZK8CmO0KGXLb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "\n",
        "#ตัดคำด้วยพจนานุกรมของ PyThaiNLP\n",
        "word_tokenize(dataset['train']['texts'][2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiSib2gx8utG"
      },
      "outputs": [],
      "source": [
        "#เปลี่ยนข้อความเป็น bag-of-words features\n",
        "tfidf = TfidfVectorizer(ngram_range=(1,2), tokenizer=word_tokenize,\n",
        "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
        "               smooth_idf=1, sublinear_tf=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJhVbJZlGWiy"
      },
      "outputs": [],
      "source": [
        "x_train = tfidf.fit_transform(dataset['train']['texts'])\n",
        "x_valid = tfidf.transform(dataset['validation']['texts'])\n",
        "x_test = tfidf.transform(dataset['test']['texts'])\n",
        "x_train,x_valid,x_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT9Ds48EGqiB"
      },
      "outputs": [],
      "source": [
        "#label คือ 0 positive; 1 neutral; 2 negative; 3 question\n",
        "enc = LabelEncoder()\n",
        "y_train = enc.fit_transform(np.array(dataset['train']['labels']))\n",
        "y_valid = enc.transform(np.array(dataset['validation']['labels']))\n",
        "y_test = enc.transform(np.array(dataset['test']['labels']))\n",
        "y_train.shape, y_valid.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5Q7_FCoGyau"
      },
      "outputs": [],
      "source": [
        "#เทรนด้วย logistic regression; ไม่มีการ pretrain ใดๆ\n",
        "model = LogisticRegression(penalty='l2', C=1.0)\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0YBJ3VGGI-F"
      },
      "outputs": [],
      "source": [
        "#ได้ผลดีกว่า Wangchanberta ที่เทรนแค่ 1 epoch อีก!\n",
        "#เรื่องนี้สอนให้รู้ว่า บางครั้งเราไม่ต้องใช้โมเดลที่ดีที่สุดเพื่อทำ task ง่ายๆก็ได้\n",
        "#โมเดลที่ซับซ้อนที่สุดอาจจะทำให้เราได้ผลดีที่สุด แต่ก็ย่อมใช้เวลาและทรัพยการมากกว่าเช่นกัน\n",
        "class Preds:\n",
        "    label_ids = y_test\n",
        "    predictions = model.predict_proba(x_test)\n",
        "\n",
        "pd.DataFrame.from_dict(classification_metrics(Preds),orient='index').transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaNmSc41PIhW"
      },
      "source": [
        "## <font color='saddlebrown'>Tabular Data Classification with scikit-learn and Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PUTipjAfgEU"
      },
      "source": [
        "จำแนกการตอบสนองของลูกค้ากับแคมเปญที่บริษัทจัดด้วยชุดข้อมูล [Retail Transaction Data](https://www.kaggle.com/regivm/retailtransactiondata) จัดทำโดย [Tanat Iempreedee](https://th.linkedin.com/in/tanat-iempreedee-67001947) ดูตัวอย่างการวิเคราะห์เต็มๆได้ที่ [Campaign Response Model - Example.ipynb](https://colab.research.google.com/drive/1eTVlSYcwUYkYDW6sPTY8XVO1QMhr-DNT?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3Bo3Q5cdL7Y"
      },
      "outputs": [],
      "source": [
        "#download data\n",
        "!wget https://github.com/ai-builders/curriculum/raw/main/data/Retail_Data_Transactions.csv\n",
        "!wget https://github.com/ai-builders/curriculum/raw/main/data/Retail_Data_Response.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_i0s5UgN9CQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwlm-n8dN-AA"
      },
      "outputs": [],
      "source": [
        "#จัดการข้อมูลให้อยู่ในรูปแบบ (X, y)\n",
        "df_response = pd.read_csv('Retail_Data_Response.csv')\n",
        "df_trans = pd.read_csv('Retail_Data_Transactions.csv', parse_dates=['trans_date'])\n",
        "\n",
        "campaign_date = datetime(2015,3,17)\n",
        "df_trans['age'] = (campaign_date - df_trans.trans_date).dt.days"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbhIAy3QOzH1"
      },
      "outputs": [],
      "source": [
        "df = df_trans.groupby('customer_id').agg(\n",
        "    recency=('age', 'min'),\n",
        "    frequency=('customer_id', 'count'),\n",
        "    monetary=('tran_amount', 'sum'),\n",
        "    tenure=('age', 'max'),\n",
        "    length_of_stay=('age', lambda x: x.max() - x.min())\n",
        ")\n",
        "df['ticket_size'] = df.monetary / df.frequency\n",
        "df['std_ticket_size'] = df_trans.groupby('customer_id').tran_amount.std()\n",
        "df['cv_ticket_size'] = df.std_ticket_size / df.ticket_size\n",
        "\n",
        "# the number of months visited\n",
        "active_months = df_trans.groupby(['customer_id', pd.Grouper(key='trans_date',freq='M')]).size().reset_index()\n",
        "active_months = active_months.groupby('customer_id').trans_date.nunique()\n",
        "df['active_months'] = active_months\n",
        "df['avg_spend_m'] = df.monetary / df.active_months\n",
        "df['avg_visit_m'] = df.frequency / df.active_months\n",
        "# done with active_months, then drop it\n",
        "df = df.drop(columns='active_months')\n",
        "df['avg_tte'] = df.length_of_stay / (df.frequency - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK0xp4NbPA3c"
      },
      "outputs": [],
      "source": [
        "#หน้าตาชุดข้อมูลตาราง\n",
        "all_data = df_response.join(df, how='inner', on='customer_id').set_index('customer_id')\n",
        "print(all_data.shape)\n",
        "all_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTdBJ3iQQPoE"
      },
      "outputs": [],
      "source": [
        "#split ข้อมูลเป็น train, validation และ test\n",
        "random_state = 1024\n",
        "test_size = 0.2\n",
        "val_size = 0.2\n",
        "\n",
        "X = all_data.drop(columns='response')\n",
        "y = all_data.response\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                            stratify=y, random_state=random_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size),\n",
        "                                                  stratify=y_train_val, random_state=random_state)\n",
        "\n",
        "train_data = X_train.join(y_train)\n",
        "val_data = X_val.join(y_val)\n",
        "test_data = X_test.join(y_test)\n",
        "\n",
        "print(f\"Train:      {len(train_data)} {len(train_data)/len(all_data):.3%}\")\n",
        "print(f\"Validation: {len(val_data)} {len(val_data)/len(all_data):.3%}\")\n",
        "print(f\"Test:       {len(test_data)} {len(test_data)/len(all_data):.3%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI2KTzXOSiTD"
      },
      "outputs": [],
      "source": [
        "X_val.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBs8f9jNRunt"
      },
      "outputs": [],
      "source": [
        "#สร้างโมเดลต้นไม้ด้วย xgboost (gradient boosted trees)\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_params = {'objective': 'binary:logistic',\n",
        "              'random_state': 1024,\n",
        "              'eval_metric': 'auc',\n",
        "              'early_stopping_rounds': 10,\n",
        "              'scale_pos_weight' : (len(y_train) - sum(y_train)) / sum(y_train),\n",
        "              'max_depth': 3,\n",
        "              'booster':'gbtree',\n",
        "              }\n",
        "\n",
        "model = xgb.XGBClassifier(**xgb_params)\n",
        "model = model.fit(X_train,\n",
        "                  y_train,\n",
        "                  eval_set=[(X_val,y_val)],\n",
        "                  verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HmaRQ2qeB5N"
      },
      "outputs": [],
      "source": [
        "#รายงานผล\n",
        "print(classification_report(y_test,model.predict_proba(X_test)[:,1].round()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT6Fr5XDemN_"
      },
      "outputs": [],
      "source": [
        "roc_auc_score(y_test, model.predict_proba(X_test)[:,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpVuvuweBSgr"
      },
      "source": [
        "## <font color='saddlebrown'>Tabular Data Classification with AutoGluon Baselines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeeG3x17gijY"
      },
      "source": [
        "ใช้ package สำหรับทำ AutoML (ทดลอง models และ hyperparameters หลากหลายให้เองโดยไม่ต้องใช้คนทำ) สำหรับปัญหาเดียวกัน\n",
        "\n",
        "ไปทดลองใช้ [AutoGluon](https://auto.gluon.ai/) เพิ่มเติมกันได้ ใช้ได้ไม่เฉพาะกับข้อมูลตารางเท่านั้นแต่ได้กับข้อมูลรูปและข้อความด้วย"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utaC9H2jBWUU"
      },
      "outputs": [],
      "source": [
        "#load autogluon library\n",
        "from autogluon.tabular import TabularPredictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLpYIi_UVH4f"
      },
      "outputs": [],
      "source": [
        "#สร้างโมเดล\n",
        "predictor = TabularPredictor(label='response',\n",
        "                             eval_metric='f1_micro').fit(train_data=train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmrZCSxwXK9x"
      },
      "outputs": [],
      "source": [
        "#รายงานผล\n",
        "#ในกรณีนี้ การใช้โมเดลหลายๆโมเดลมาช่วยกันทำนาย (ensemble) ทำให้ได้ผลดีขึ้นมาก\n",
        "predictor.leaderboard(test_data, silent=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmG2ie8XlRn2"
      },
      "source": [
        "# <font color='darkgreen'>Checkpoint ท้ายบท"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWSEmKilzjSC"
      },
      "source": [
        "## คำถามชวนคิดเกี่ยวกับบทเรียน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GugwcUJbhF9j"
      },
      "source": [
        "1. รอบตัวของคุณมีอะไรที่เป็น AI แบบ 1) Rule-based Systems และ 2) Machine Learning Systems บ้าง จงยกตัวอย่างมาอย่างละ 3 อัน\n",
        "\n",
        "2. คุณได้เรียนรู้ส่วนประกอบต่างๆของ machine learning model คุณคิดว่าส่วนประกอบไหนมีความสำคัญที่สุดเพื่อให้ได้ผลลัพธ์การทำนายที่ดีที่สุดระหว่าง `Inputs/Labels` (ชุดข้อมูล), `Weights` (ตัวโมเดล), `Loss Function`, `Optimizer` และเพราะอะไร\n",
        "\n",
        "3. ในโครงงานที่คุณอยากจะทำ คุณคิดว่าจะแบ่ง train-validation-test splits อย่างไร เพราะอะไร\n",
        "\n",
        "4. คุณจะเห็นได้ว่าการเทรน machine learning model สามารถทำได้ด้วยเครื่องมือหลากหลายชนิด แต่ละเครื่องมือก็มี 1) ความยากง่ายในการใช้งาน 2) ความสามารถในการปรับแต่ง 3) ความมีประสิทธิภาพ ที่แตกต่างกัน 4) เหมาะกับข้อมูลรูปแบบต่างกัน (images, texts, tabular data) คุณคิดว่าเราควรเลือกใช้เครื่องมือเหล่านี้อย่างไรเมื่อคำนึงถึง 1)-4)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2QKHcGUpdlP"
      },
      "source": [
        "## <font color='saddlebrown'>ทบทวนคำศัพท์</font>\n",
        "\n",
        "1. **<font color='darkblue'>`Inputs`</font>** (ชื่อเรียกอื่น: Features, Independent Variables, \"X\") - ข้อมูลที่ใส่เข้าไปในโมเดลเพื่อทำการเทรน/เพื่อให้ได้ `Predictions`\n",
        "\n",
        "2. **<font color='darkblue'>`Labels`</font>** (ชื่อเรียกอื่น: Targets, Dependent Variables, Ground Truth, \"y_true\") - ข้อมูลที่โมเดลกำลังพยายามทำนาย\n",
        "\n",
        "3. **<font color='darkblue'>`Predictions`</font>** (ชื่อเรียกอื่น: Outputs, \"y_predict\") - ผลการทำนายจากโมเดล ส่วนใหญ่สำหรับ classification จะเป็น probabilities สำหรับแต่ละ label สำหรับ regression จะเป็นจำนวนจริง\n",
        "\n",
        "4. **<font color='darkblue'>`Weights`</font>** (ชื่อเรียกอื่น: Parameters, Coefficients, Models, \"W, b\") - ค่าของฟังชั่นสำหรับเปลี่ยน `Inputs` เป็น `Predictions`; รูปแบบของ `Weights` (รูปแบบสมการ) มีหลากหลายชนิด เรียกว่า `Architecture` หลายครั้งเวลาพูดถึง ML models เราหมายถึงส่วน `Weights` ที่ถูกเทรนด้วยข้อมูลเรียบร้อยแล้วนั่นเอง\n",
        "\n",
        "5. **<font color='darkblue'>`Loss Function`/`Loss`</font>** (ชื่อเรียกอื่น: Cost Function, Objective Function; แต่ Objective Function จะอยากให้เยอะหรือน้อยก็ได้) - ฟังชั่นสำหรับเปลี่ยน `Labels` และ `Predictions` ให้เป็นค่า `Loss`; `Loss` ยิ่งน้อยยิ่งแสดงว่าโมเดลทำได้ดี\n",
        "\n",
        "6. **<font color='darkblue'>`Gradients`</font>** - ค่า partial derivative ของ `Loss` ด้วย `Weights`\n",
        "\n",
        "7. **<font color='darkblue'>`Optimizer`</font>** - class สำหรับทำการ update `Weights` ด้วย `Gradients` (อาจจะเป็นการบวก-ลบ-คูณ-หาร `Weights` ปัจจุบันด้วย `Gradients`)\n",
        "\n",
        "8. **<font color='darkblue'>`Training Loop`</font>** - การส่ง `Inputs` และ `Labels` ให้โมเดลเรียนรู้ด้วยการ update `Weights`\n",
        "\n",
        "9. **<font color='darkblue'>`Example`</font>** - ตัวอย่างในการเทรนโมเดล โดยทั่วไปประกอบด้วยคู่ `Inputs` และ `Labels` (ในกรณี ML ชนิดที่เราสนใจคือ [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning)\n",
        "\n",
        "10. **<font color='darkblue'>`Batch`/`Batch Size`</font>** - `Example` จำนวน X อัน โดย X คือ `Batch Size`\n",
        "\n",
        "11. **<font color='darkblue'>`Dataloader`</font>** - class สำหรับส่ง `Batch` ไปให้โมเดลเทรน ในบางกรณี เราจำเป็นต้องส่งข้อมูลไปทีละ `Batch` เรื่องจากข้อมูลทั้งหมดใหญ่เกินที่จะใส่เข้าไปใน memory ของเครื่อง\n",
        "\n",
        "12. **<font color='darkblue'>`Iteration`</font>** - การที่โมเดลเรียนรู้ 1 `Batch` จนครบ `Training Loop`\n",
        "\n",
        "13. **<font color='darkblue'>`Epoch`</font>** - การที่โมเดลเรียนรู้จาก `Example` ทั้งหมด 1 รอบ\n",
        "\n",
        "14. **<font color='darkblue'>`Trainer`</font>** - class สำหรับจัดการ `Training Loop`\n",
        "\n",
        "15. **<font color='darkblue'>`Logger`</font>** - class สำหรับจดบันทึกข้อมูลการเทรน\n",
        "\n",
        "16. **<font color='darkblue'>`Metric`</font>** - ตัวชี้วัดว่าโมเดลทำงานได้ดีแค่ไหน โดยส่วนใหญ่เป็นคนละฟังชั่นกับ `Loss Function`\n",
        "\n",
        "17. **<font color='darkblue'>`Train Set`</font>** - ชุดข้อมูลที่ป้อนให้โมเดลเพื่อทำการเรียนรู้ตาม `Training Loop` (\"แบบฝึกหัด\")\n",
        "\n",
        "18. **<font color='darkblue'>`Validation Set`</font>** (ชื่อเรียกอื่น: Development Set, Dev Set) - ชุดข้อมูลสำหรับคำนวณ metric เพื่อเลือกโมเดลที่ดีที่สุด เพราะในความเป็นจริงเราอาจจะเทรนโมเดลหลากหลายสถาปัตยกรรมหรือ hyperparameters; metric ที่ถูกคำนวณบน `validation` จึงใช้สำหรับเปรียบเทียบโมเดลที่ดีที่สุด (\"ข้อสอบ pre-test จากโรงเรียนกวดวิชา\")\n",
        "\n",
        "19. **<font color='darkblue'>`Test Set`</font>** - ชุดข้อมูลสำหรับคำนวณ metric ของโมเดลที่เราเลือกแล้วว่าดีที่สุด เปรียบเสมือนตัวชี้วัดว่าโมเดลของเราจะทำได้ดีแค่ไหนในโลกแห่งความเป็นจริง; ชุดข้อมูลนี้จะถูกรันเพียงครั้งเดียวเท่านั้นในการทำโครงงาน ด้วยโมเดลที่ดีที่สุดที่เราเลือกมาจาก `validation` (\"ข้อสอบจริง\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
