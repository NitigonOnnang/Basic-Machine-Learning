{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjJ7qBLsdWq5"
      },
      "source": [
        "# บทที่ 2 - ชุดข้อมูลมหัศจรรย์และถิ่นที่อยู่"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdhweOfHROfi"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ai-builders/curriculum/blob/main/notebooks/02_fantastic_datasets.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udDyKuYVRS0Y"
      },
      "source": [
        "ในปัจจุบันชุดข้อมูลที่มีพร้อมทั้งปริมาณและคุณภาพเป็นส่วนสำคัญในการสร้าง ML models ในบทเรียนนี้เราจะเรียนรู้วิธีการหาข้อมูลมาเทรนโมเดลของเราทั้งจากชุดข้อมูล open data, web scraping, หรือสร้างขึ้นมาเองจากโมเดลและโค้ด open source ทั้งนี้การหาข้อมูลมาเทรนโมเดลจากแหล่งข้อมูลสาธารณะที่กล่าวมานั้นเราต้องให้ความสำคัญเรื่องลิขสิทธิ์และจริยธรรม (แม้แต่โมเดลเองก็สร้างข้อมูลที่ผิดลิขสิทธิ์-จริยธรรมได้; เรียนเพิ่มเติมในบทที่ 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lfp5-j71dLJ2"
      },
      "source": [
        "## ติดตั้ง Package ที่ต้องใช้งาน"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVfe2zgZdNaE"
      },
      "outputs": [],
      "source": [
        "#fastai\n",
        "!pip install -q fastbook==0.0.29\n",
        "\n",
        "#ctgan\n",
        "!pip install -q sdv==0.18.0\n",
        "\n",
        "#review generation\n",
        "!pip install -q transformers==4.27.3\n",
        "\n",
        "#DuckDuckGo Search API\n",
        "!pip install -q duckduckgo_search==5.3.0b4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MElLEVboe23U"
      },
      "source": [
        "## Data-centric AI เมื่อปริมาณ-คุณภาพข้อมูลสำคัญเท่ากับหรือมากกว่าคุณภาพโมเดล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA9SWMpwTTsI"
      },
      "source": [
        "ในปัจจุบันเรามีสถาปัตยกรรมประสิทธิภาพสูงมากมายให้เลือกใช้ทำ ML models (images - ResNet/EfficientNet, texts - BERT family, tabular data - gradient boosted trees) ปริมาณและคุณภาพของชุดข้อมูลจึงเริ่มมีความสำคัญมากยิ่งขึ้นในการทำให้ประสิทธิภาพของโมเดลของเราดีขึ้น\n",
        "\n",
        "แนวคิด Data-centric AI ถูกทำให้เป็นที่รู้จักโดย [Andrew Ng และ Landing AI](https://landing.ai/data-centric-ai/) กล่าวคือแทนที่เราจะพยามสร้างโมเดลที่มีประสิทธิภาพยิ่งขึ้น (และโดยส่วนใหญ่แล้วซับซ้อน ใหญ่ และใช้งานยากขึ้น)ให้เรามาใช้เวลาในการ 1) เพิ่มปริมาณข้อมูลและ 2) พัฒนาคุณภาพข้อมูลแทน\n",
        "\n",
        "ยกตัวอย่าง Data-centric AI ในการตัดคำภาษาไทยด้วยโมเดลโดย [@kornwtp](https://medium.com/@kornwtp/) ในบทความ [Data-Centric ML for Thai Word Segmentation](https://medium.com/@kornwtp/data-centric-for-thai-word-segmentation-e31a79db650d) พวกเขาทำการทดลองตัดคำภาษาไทยด้วยชุดข้อมูล [VISTEC-TP-TH-2021](https://github.com/mrpeerat/OSKut/tree/main/VISTEC-TP-TH-2021) (ประมาณ 5 หมื่นประโยค) ด้วยโมเดล [deepcut](https://github.com/rkcosmos/deepcut) อย่างที่เห็นในตารางรายงานผล การเพิ่มขนาดโมเดลถึง 1 เท่าตัวมีผลให้ความแม่นยำเพิ่มขึ้นเพียงแค่ 1.37% กลับกันการเพิ่มข้อมูลเพียงแค่ 1,500 ตัวอย่างทำให้ความแม่นยำเพิ่มขึ้นถึง 4.90% ยิ่งถ้าข้อมูลที่เพิ่มถูกทำความสะอาดแล้วจะทำให้เพิ่มขึ้นถึง 7.48% (แน่นอนว่าถ้ายิ่งโมเดลใหญ่และข้อมูลเยอะก็ยิ่งดี)\n",
        "\n",
        "<img src=https://miro.medium.com/max/1400/0*J2vA5iRnIMfuDDAc width=\"500px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbSBbhneGbF"
      },
      "source": [
        "## ชุดข้อมูลสาธารณะ - Open Data and Open Source"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58a6xs9YFtM"
      },
      "source": [
        "ชุดข้อมูล open data และโค้ด open source ถือเป็นสาธารณูปโภคขั้นพื้นฐานของการทำ ML และโชคดีที่น่าจะเป็นสาธารณูปโภคขั้นพื้นฐานไม่กี่อย่างที่เราในฐานะบุคคลสามารถช่วยกันสร้างได้ด้วยการนำไปใช้-เผยแพร่ชุดข้อมูลและโค้ด\n",
        "\n",
        "โดยทั่วไปแล้ว ชุดข้อมูลจะถูกเผยแพร่ภายใต้ [Creative Commons Licenses](https://creativecommons.org/) ที่นิยมดังต่อไปนี้\n",
        "\n",
        "* `CC` - Creative Commons\n",
        "* `BY` (Attribution) - ใช้ทำซ้ำ แจกจ่าย ดัดแปลงได้ แต่ต้องอ้างอิงเจ้าของผลงาน\n",
        "* `SA` (ShareAlike) - ใช้ทำซ้ำ แจกจ่าย ดัดแปลงได้ แต่ต้องคง License เดิมไว้\n",
        "* `NC` (NonCommercial) - ใช้ทำซ้ำ แจกจ่าย ดัดแปลงได้ แต่ห้ามใช้เพื่อการค้า\n",
        "* `ND` (NoDerivatives) - ใช้ทำซ้ำ แจกจ่ายได้ แต่ห้ามดัดแปลง\n",
        "\n",
        "เราแนะนำให้คุณแบ่งปันผลงานเป็น `CC-BY-SA` เพื่อให้ชุมชนนักพัฒนาสามารถนำไปต่อยอดได้ เหมือนเช่นที่เรานำงานของชุมชนมาต่อยอด\n",
        "\n",
        "คุณสามารถเลือก License ที่คุณต้องการได้[ที่นี่](https://creativecommons.org/choose/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd0xJTVItcjm"
      },
      "source": [
        "### แหล่งค้นหาชุดข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SifTEta5aiFK"
      },
      "source": [
        "ต่อไปนี้คือแหล่งข้อมูลหลักๆที่คุณสามารถพบเจอชุดข้อมูล open data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x41zHq7lvDcm"
      },
      "source": [
        "แหล่งค้นหาชุดข้อมูลทั่วไป\n",
        "* [Kaggle](https://www.kaggle.com/datasets)\n",
        "* [Google Dataset Search](https://datasetsearch.research.google.com/)\n",
        "* [Papers with Code](https://paperswithcode.com/datasets)\n",
        "* [Tensorflow Datasets](https://www.tensorflow.org/datasets)\n",
        "\n",
        "แหล่งค้นหาชุดข้อมูล Images\n",
        "* [Open Images Dataset](https://storage.googleapis.com/openimages/web/index.html)\n",
        "* [torchvision.datasets](https://pytorch.org/vision/stable/datasets.html)\n",
        "\n",
        "แหล่งค้นหาชุดข้อมูล NLP และ Speech\n",
        "* [Hugging Face Datasets](https://huggingface.co/datasets)\n",
        "* [torchtext.datasets](https://pytorch.org/text/stable/datasets.html)\n",
        "* [torchaudio.datasets](https://pytorch.org/audio/stable/datasets.html)\n",
        "* [NLP for Thai](https://nlpforthai.com/)\n",
        "\n",
        "\n",
        "แหล่งค้นหาชุดข้อมูล Tabular Data\n",
        "* [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7dRu-mGtsOt"
      },
      "source": [
        "### ชื่อ Task ไว้ใช้ค้นหาชุดข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npLBrq4Aatej"
      },
      "source": [
        "อีกแนวทางการค้นหาข้อมูลคือค้นหา \"[ชื่อ task] datasets\" ใน search engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IVRY-Gjtx94"
      },
      "source": [
        "Images\n",
        "* Image classification; จำแนกรูปภาพ\n",
        "* Object detection; จับวัตถุในรูปภาพ\n",
        "* Image segmentation; จำแนก pixel ในรูปภาพ\n",
        "* Image information retrieval (search); ค้นหารูปภาพด้วยรูปภาพ\n",
        "* Image captioning; ใส่คำบรรยายให้รูปภาพ\n",
        "* Visual question answering; ตอบคำถามจากรูปภาพ\n",
        "* Image generation; สร้างรูปภาพ\n",
        "* Image reconstruction; ซ่อมแซมรูปภาพ (รูปเก่า, เซนเซอร์ ฯลฯ)\n",
        "* Style transfer; เปลี่ยนสไตล์ของรูปภาพ\n",
        "\n",
        "NLP\n",
        "* Sequence classification; จำแนกข้อความ\n",
        "* Token classification; จำแนกหน่วยคำในข้อความ\n",
        "* Extractive question answering; ตอบคำถามจากบทความ\n",
        "* Machine translation; แปลภาษา\n",
        "* Summarization; ย่อความ\n",
        "* Paraphrasing; ถอดความ\n",
        "* Optical character recognition; เปลี่ยนตัวอักษรในภาพเป็นข้อความ\n",
        "* Text information retrieval (search); ค้นหาข้อความด้วยข้อความ\n",
        "* Text generation; สร้างข้อความ\n",
        "* Dialogue; สร้างบทสนทนาตอบโต้\n",
        "\n",
        "Speech\n",
        "* Speech classification; จำแนกเสียง\n",
        "* Automatic speech recognition; ถอดเสียงเป็นข้อความ\n",
        "* Speech synthesis; สร้างเสียง (จากข้อความ)\n",
        "* Speaker recognition; จำแนกผู้พูด\n",
        "\n",
        "Tabular Data\n",
        "* Multi-class classification; จำแนกตัวอย่าง (1 ตัวอย่าง = 1 ประเภท)\n",
        "* Multi-label classification; จำแนกตัวอย่าง (1 ตัวอย่าง = มากกว่า 1 ประเภท)\n",
        "* Regression; ทำนายจำนวนจริง\n",
        "* Time series forecasting; ทำนายตัวเลขในเชิงเวลา\n",
        "* Recommendation; แนะนำสิ่งของ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFjggxS9jTU2"
      },
      "source": [
        "## หารูปภาพจาก DuckDuckGo Image Search API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXlc2pNnHeR7"
      },
      "source": [
        "เทคนิคแนะนำโดย [fastai v4 part1 lesson 2](https://github.com/fastai/fastbook/blob/master/02_production.ipynb) ผู้ที่สนใจสามารถไปติดตามบทเรียนต้นทางได้ที่ [course.fast.ai](https://course.fast.ai/) สามารถใช้ search engine อื่นๆ เช่น Google, Bing เป็นต้น เราเลือกใช้ DuckDuckGo เนื่องจากความสะดวกที่สามารถส่งคำขอค้นหารูปภาพได้โดยไม่ต้องใช้ API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2G1YaxhTjUC8"
      },
      "outputs": [],
      "source": [
        "from fastbook import *\n",
        "from fastai.vision.widgets import *\n",
        "from tqdm.auto import tqdm\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "#download from duckduckgo\n",
        "def search_images_ddg(key, max_n=200):\n",
        "     \"\"\"Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images\n",
        "        (Adopted from https://github.com/deepanprabhu/duckduckgo-images-api)\n",
        "     \"\"\"\n",
        "     url        = 'https://duckduckgo.com/'\n",
        "     params     = {'q':key}\n",
        "     res        = requests.post(url,data=params)\n",
        "     searchObj  = re.search(r'vqd=([\\d-]+)\\&',res.text)\n",
        "     if not searchObj: print('Token Parsing Failed !'); return\n",
        "     requestUrl = url + 'i.js'\n",
        "     headers    = {'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:71.0) Gecko/20100101 Firefox/71.0'}\n",
        "     params     = (('l','us-en'),('o','json'),('q',key),('vqd',searchObj.group(1)),('f',',,,'),('p','1'),('v7exp','a'))\n",
        "     urls       = []\n",
        "     while True:\n",
        "         try:\n",
        "             res  = requests.get(requestUrl,headers=headers,params=params)\n",
        "             data = json.loads(res.text)\n",
        "             for obj in data['results']:\n",
        "                 urls.append(obj['image'])\n",
        "                 max_n = max_n - 1\n",
        "                 if max_n < 1: return L(set(urls))     # dedupe\n",
        "             if 'next' not in data: return L(set(urls))\n",
        "             requestUrl = url + data['next']\n",
        "         except:\n",
        "             pass\n",
        "\n",
        "def search_images_ddg_v2(key,max_n=200):\n",
        "    \"\"\"Search for 'key' with DuckDuckGo and return a unique urls of 'max_n' images\n",
        "        Note: Instead of using our own implementation, we using library instead!\n",
        "    \"\"\"\n",
        "    responses = DDGS().images(\n",
        "                          keywords=key,\n",
        "                          region=\"wt-wt\",\n",
        "                          safesearch=\"off\",\n",
        "                          size=None,\n",
        "                          color=None,\n",
        "                          type_image=None,\n",
        "                          layout=None,\n",
        "                          license_image=None,\n",
        "                          max_results=max_n,\n",
        "                      )\n",
        "    urls = [ r['image'] for r in responses ]\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0lSJEF5nJVw"
      },
      "outputs": [],
      "source": [
        "#save ไว้ที่ไหน\n",
        "path = Path('images')\n",
        "#เอารูปอะไรบ้าง (labels)\n",
        "labels = ['chocolate chip cookies','raisin cookies']\n",
        "#เอา label ละกี่รูป\n",
        "MAX_N = 100\n",
        "\n",
        "if not path.exists():\n",
        "    path.mkdir()\n",
        "for l in tqdm(labels):\n",
        "    dest = (path/l)\n",
        "    dest.mkdir(exist_ok=True)\n",
        "    results = search_images_ddg_v2(l, max_n=MAX_N)\n",
        "    download_images(dest, urls=results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd81V-kOFVMv"
      },
      "outputs": [],
      "source": [
        "#เช็คว่าได้รูปอะไรบ้าง\n",
        "fns = get_image_files(path)\n",
        "fns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkkKMcdZFbFz"
      },
      "outputs": [],
      "source": [
        "#ดูว่าหารูปไหนไม่เจอบ้าง\n",
        "failed = verify_images(fns)\n",
        "failed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dru0L1cdFbIr"
      },
      "outputs": [],
      "source": [
        "#นำรูปที่หาไม่เจอออก\n",
        "failed.map(Path.unlink)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y_208ZQj6lZ"
      },
      "source": [
        "## Web Scraping ดึงข้อมูลจากเว็บไซต์สาธารณะ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymkPelcdGVTD"
      },
      "source": [
        "Web Scraping คือการเขียนโปรแกรมดึงข้อมูลจากเว็บไซต์สาธารณะ ข้อควรระวังคือแม้โดยความเป็นจริงแล้วข้อมูลเหล่านั้นจะถูกเปิดเผยให้ใครเข้าไปดูก็ได้ แต่ไม่ได้หมายความว่าเจ้าของเว็บไซต์จะอยากให้เราดึงข้อมูลจำนวนมากจากเว็บไซต์ของพวกเขา ไม่ว่าจะด้วยเหตุผลด้านลิขสิทธิ์การใช้งานหรือการเพิ่มภาระให้เซิร์ฟเวอร์ของเว็บไซต์นั้นๆ ยกตัวอย่างเช่น Facebook หรือ Twitter ที่ข้อตกลงการใช้งานของพวกเขาไม่อนุญาตให้เราดึงข้อมูลด้วย Web Scraping แต่อนุญาตให้ดึงข้อมูลผ่าน API อย่างเป็นทางการแทน\n",
        "\n",
        "หากไม่แน่ใจในนโยบายการใช้งาน ทางที่ดีควรติดต่อขออนุญาตจากเจ้าของเว็บไซต์ก่อน นี่คือตัวอย่างที่ทางทีมงานติดต่อไปยัง [soccersuck.com](https://www.soccersuck.com) เพื่อขออนุญาตนำมาใช้สอน\n",
        "\n",
        "> ทีมงาน AI Builders: สวัสดีครับ ผมทำโครงการสอนเด็กมัธยมทำ AI ชื่อ AI Builders (https://www.facebook.com/aibuildersx) อยู่ครับ พอดีครั้งนี้เราจะจัดสอนช่วงหลังสงกรานต์ และมีหัวข้อหนึ่งคือ web scraping หรือการเขียนโปรแกรมดึงเนื้อหาจากเว็บไซต์ อยากขออนุญาตใช้ https://www.soccersuck.com/boards/oldnews เป็นตัวอย่างในการฝึกสอนครับ โดนสอนให้น้องๆเขียนโปรแกรมดึงพาดหัวข่าว, url ข่าว, และเนื้อข่าวเฉพาะกระทู้ที่อยู่ใน old news 3 หน้าเท่านั้น โปรแกรมทั้งหมดใช้เพื่อสาธิตในการศึกษาครับ นักเรียนมีจำนวน 50 คน คิดว่าไม่มีปัญหา traffic แม้จะกดรันโค้ดพร้อมกันทุกคนครับ\n",
        "\n",
        "> Soccersuck: ได้ครับไม่มีปัญหา\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5NDzNrGQjw"
      },
      "source": [
        "### Static Web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd0Q6VKNteI0"
      },
      "source": [
        "สมมุติว่าเราอยากทำโมเดล \"เขียนพาดหัวข่าวฟุตบอลจากเนื้อข่าวในเว็บ [soccersuck.com](https://www.soccersuck.com/boards/oldnews)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIbJQ1B3j7St"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1oRwKwurXS8"
      },
      "source": [
        "#### ดึง HTML มาเปลี่ยนเป็น Soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1imBwROfwPh"
      },
      "outputs": [],
      "source": [
        "#package สำหรับจัดการ html เรียกว่า BeautifulSoup\n",
        "def get_soup(url):\n",
        "    with requests.get(url) as r:\n",
        "        soup = BeautifulSoup(r.text, features='html.parser')\n",
        "    return soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9QLFWVeqrY_"
      },
      "outputs": [],
      "source": [
        "#ดึงข้อมูล html จากเว็บไซต์มาเปลี่ยนเป็น soup\n",
        "url = f'https://www.soccersuck.com/boards/oldnews/1/'\n",
        "soup = get_soup(url)\n",
        "soup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Ekl-ihrdWn"
      },
      "source": [
        "#### หา Tag ต่างๆใน Soup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOXnPRISp38F"
      },
      "outputs": [],
      "source": [
        "#หา tag ชื่อ div ที่มี class oldnew_p_tr\n",
        "#เราสามาร?ถหาโดย id ได้ด้วย แค่เปลี่ยนเป็น id='id_you_are_looking_for'\n",
        "soup.find('div', class_='oldnew_p_tr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v74oubsPqBL-"
      },
      "outputs": [],
      "source": [
        "soup.find('div', class_='oldnew_p_tr').find('a')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iCzLh8sqED7"
      },
      "outputs": [],
      "source": [
        "#เอา text จาก tag <a>\n",
        "soup.find('div', class_='oldnew_p_tr').find('a').text.strip(),\\\n",
        "#เอา href จาก tag <a>\n",
        "soup.find('div', class_='oldnew_p_tr').find('a').get('href')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzKN_3GlfwVp"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame([\n",
        "{'headline_abridged': i.text.strip(),\n",
        " 'url': i.find('a').get('href')} \\\n",
        " #find() จะหาแค่ tag แรกที่เจอ แต่ find_all() จะหาทุก tag ที่ตรงเงื่อนไข\n",
        " for i in soup.find_all('div', class_='oldnew_p_tr')]).head(10)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZrL-yhxr30k"
      },
      "source": [
        "#### ดึงข้อมูลจาก url ย่อย"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EprNiQaBraln"
      },
      "outputs": [],
      "source": [
        "#จากหน้า list　ข่าว พาดหัวจะแสดงได้ไม่เต็ม\n",
        "df.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWLt7Ararhty"
      },
      "outputs": [],
      "source": [
        "#เราสามารถเข้าไปใน url ของข่าวแต่ละข่าวเพื่อไปเอาพาดหัวแบบเต็มและเนื้อข่าวได้\n",
        "soup = get_soup(df.loc[0,'url'])\n",
        "soup.find('div', class_='post_head').text.strip(),\\\n",
        "soup.find('div', class_='post_desc').text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uWIvES_sKGY"
      },
      "outputs": [],
      "source": [
        "#เขียนฟังชั่นเพื่อเข้าไปดึงข้อมูลจากหน้าข่าว\n",
        "def get_post_head_desc(url):\n",
        "    soup = get_soup(url)\n",
        "    return {'post_head': soup.find('div', class_='post_head').text.strip(),\n",
        "            'post_desc': soup.find('div', class_='post_desc').text.strip()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VGSN39CsASR"
      },
      "outputs": [],
      "source": [
        "#วนลูป (map) เพื่อไปดึงพาดหัวแบบเต็มและเนื้อข่าว\n",
        "df2 = pd.DataFrame(df.url.map(get_post_head_desc).tolist())\n",
        "df = pd.concat([df,df2],1)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTpIaLJNvjpB"
      },
      "source": [
        "### Static Web แบบเร็วด้วย Concurrency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rojt_hkQ6xQ6"
      },
      "source": [
        "บางครั้งแค่ scape ทีละ url อาจจะไม่ทันใจ เราจึงสามารถ scrape ทีละหลายๆ url \"พร้อมๆ\" กันได้ด้วย concurrency วิธีมีหลากหลาย แต่เราเลือกใช้วิธีที่แนะนำโดย [Nick Becker, RAPIDS Team at NVIDIA](https://beckernick.github.io/faster-web-scraping-python/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Fg13ztvrVL"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpy344zVvsJ-"
      },
      "outputs": [],
      "source": [
        "#url ข่าวที่เราต้องการ scrape\n",
        "urls = df.url.tolist()\n",
        "urls[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZb6Qj8u8WKv"
      },
      "outputs": [],
      "source": [
        "#ฟังชั่นเพื่อเข้าไปดึงข้อมูลจากหน้าข่าวอันเดิม\n",
        "#เพิ่มเติมคือเราจะบอกให้มันเก็บข้อมูลที่ถูก scrape ไว้ใน list ชื่อ res\n",
        "def get_post_head_desc_append(url, res):\n",
        "    soup = get_soup(url)\n",
        "    res.append({'post_head': soup.find('div', class_='post_head').text.strip(),\n",
        "            'post_desc': soup.find('div', class_='post_desc').text.strip()})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeP4ixtF8Yg9"
      },
      "outputs": [],
      "source": [
        "#เราใช้ 20 threads คือให้ดึงเต็มที่ 20 url　พร้อมๆกัน\n",
        "MAX_THREADS = 20\n",
        "\n",
        "def scrape_multithread(urls, scrape_func):\n",
        "    threads = min(MAX_THREADS, len(urls))\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
        "        executor.map(scrape_func, urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_f7gzQMFDIW"
      },
      "outputs": [],
      "source": [
        "#ทำการ scrape แบบ concurrent\n",
        "from functools import partial\n",
        "\n",
        "res = []\n",
        "scrape_multithread(urls, partial(get_post_head_desc_append, res=res))\n",
        "pd.DataFrame(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDucqGj_ft_j"
      },
      "source": [
        "จะเห็นว่าใช้เวลาเพียง 2 วินาที เทียบกับการดึงทีละ url ที่ใช้เวลากว่า 15 วินาที"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynRFATskCLnK"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = []\n",
        "scrape_multithread(urls, partial(get_post_head_desc_append, res=res))\n",
        "pd.DataFrame(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN6oIbVvD3b7"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "def get_post_head_desc(url):\n",
        "    soup = get_soup(url)\n",
        "    return {'post_head': soup.find('div', class_='post_head').text.strip(),\n",
        "            'post_desc': soup.find('div', class_='post_desc').text.strip()}\n",
        "\n",
        "pd.DataFrame(df.url.map(get_post_head_desc).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NKb9oW9EdEw"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "res = []\n",
        "for url in df.url.tolist():\n",
        "     res.append(get_post_head_desc(url))\n",
        "pd.DataFrame(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZFWviluGT_i"
      },
      "source": [
        "### Dynamic Web"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xsc2kETFroU"
      },
      "source": [
        "ในบางกรณี เว็บไซต์จำเป็นต้องให้เราทำกิจกรรมอะไรบางอย่างเพื่อเข้าถึงหน้าที่เราต้องการ scrape เช่น ต้องล็อกอินก่อน, ต้องกดบางปุ่มก่อน เป็นต้น เราเรียกเว็บไซต์ที่มีการเปลี่ยนแปลงจากการทำกิจกรรมอะไรบางอย่างเหล่านี้ว่า Dynamic Web\n",
        "\n",
        "ในกรณี soccersuck.com สมมุติว่าเราไม่รู้ url ของหน้าต่อไป จำเป็นต้องเขียนโปรแกรมให้กดหน้าต่อไปให้ เราจะใช้ [selenium](https://selenium-python.readthedocs.io) มาช่วยทำกิจกรรมเหล่านี้ (กดหน้าถัดไป)\n",
        "\n",
        "วิธีลง selenium บน colab จาก [Chalach Monkhontirapat](https://medium.com/equinox-blog/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%83%E0%B8%8A%E0%B9%89-selenium-%E0%B8%9A%E0%B8%99-google-colaboratory-984739ed44e5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXz8vkycS_AI"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "# Ubuntu no longer distributes chromium-browser outside of snap\n",
        "#\n",
        "# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n",
        "\n",
        "# Add debian buster\n",
        "cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n",
        "deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n",
        "EOF\n",
        "\n",
        "# Add keys\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n",
        "apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n",
        "\n",
        "apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n",
        "apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n",
        "apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n",
        "\n",
        "# Prefer debian repo for chromium* packages only\n",
        "# Note the double-blank lines between entries\n",
        "cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n",
        "Package: *\n",
        "Pin: release a=eoan\n",
        "Pin-Priority: 500\n",
        "\n",
        "\n",
        "Package: *\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 300\n",
        "\n",
        "\n",
        "Package: chromium*\n",
        "Pin: origin \"deb.debian.org\"\n",
        "Pin-Priority: 700\n",
        "EOF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpCxNnLohCCL"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install chromium chromium-driver\n",
        "!pip3 -q install selenium\n",
        "exit() #reset runtime เพื่อให้ package ใช้งานได้อย่างถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mhMZJtuGVy8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5YlaZygtCMT"
      },
      "source": [
        "#### Selenium ก็ Scrape Static Web ได้"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaRsRq8Th2Hn"
      },
      "outputs": [],
      "source": [
        "#ติดตั้ง web driver เพื่อจำลองผู้ใช้เว็บไซต์\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "#ฟังชั่นเปลี่ยน soup เป็น dataframe\n",
        "def get_df(soup):\n",
        "    return pd.DataFrame([{'headline_abridged': i.text.strip(),\n",
        "                          'url': i.find('a').get('href')} for i in soup.find_all('div', class_='oldnew_p_tr')])\n",
        "\n",
        "driver = webdriver.Chrome(options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na06VzYvIuYp"
      },
      "outputs": [],
      "source": [
        "#เข้าไปหน้าข่าวเพื่อเก็บ url เหมือนเดิม\n",
        "url = f'https://www.soccersuck.com/boards/oldnews'\n",
        "driver.get(url)\n",
        "soup = BeautifulSoup(driver.page_source)\n",
        "driver.close()\n",
        "\n",
        "df = pd.DataFrame([\n",
        "{'headline_abridged': i.text.strip(),\n",
        " 'url': i.find('a').get('href')} for i in soup.find_all('div', class_='oldnew_p_tr')]).head(10)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ewNi9GDtGy7"
      },
      "source": [
        "#### Selenium สำหรับ Dynamic Web"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlgpIPLmMmi9"
      },
      "outputs": [],
      "source": [
        "#ติดตั้ง web driver เพื่อจำลองผู้ใช้เว็บไซต์\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "#ฟังชั่นเปลี่ยน soup เป็น dataframe\n",
        "def get_df(soup):\n",
        "    return pd.DataFrame([{'headline_abridged': i.text.strip(),\n",
        "                          'url': i.find('a').get('href')} for i in soup.find_all('div', class_='oldnew_p_tr')])\n",
        "\n",
        "driver = webdriver.Chrome('chromedriver',options=chrome_options)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a--qUdKBKBWD"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "\n",
        "#เปิดหน้าเว็บตาม url\n",
        "url = f'https://www.soccersuck.com/boards/oldnews'\n",
        "driver.get(url)\n",
        "print(f'Opened {url}')\n",
        "\n",
        "#เก็บข้อมูล soup และ dataframe จากหน้า 1\n",
        "soup = BeautifulSoup(driver.page_source)\n",
        "d = get_df(soup)\n",
        "d['page'] = '1'\n",
        "dfs.append(d)\n",
        "print(f'Saved soup and dataframe from page 1')\n",
        "\n",
        "#เราจะกดปุ่มหน้า 2 และ 3\n",
        "for i in ['2','3']:\n",
        "\n",
        "    #เช็คดูว่าปุ่มหน้าที่จะกด ('2' และ '3') โหลดแล้วหรือยัง\n",
        "    try:\n",
        "        element = WebDriverWait(driver, 10).until(\n",
        "            EC.visibility_of_element_located((By.LINK_TEXT, i))\n",
        "        )\n",
        "        print(f'Pagination button {i} found!')\n",
        "    except:\n",
        "        print(f'Pagination button {i} NOT found')\n",
        "\n",
        "    #คลิกไปหน้าถัดไป\n",
        "    link = driver.find_element(By.LINK_TEXT, i)\n",
        "    link.click()\n",
        "    print(f'Clicked pagination button {i}')\n",
        "\n",
        "    #เก็บข้อมูล soup และ dataframe จากหน้า i\n",
        "    soup = BeautifulSoup(driver.page_source)\n",
        "    d = get_df(soup)\n",
        "    d['page'] = i\n",
        "    dfs.append(d)\n",
        "    print(f'Saved soup and dataframe from page {i}')\n",
        "\n",
        "#ปิดหน้า web\n",
        "driver.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_IuOPPeJyjJ"
      },
      "outputs": [],
      "source": [
        "df = pd.concat(dfs).reset_index(drop=True)\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOoMIytZknOO"
      },
      "source": [
        "## สร้างขึ้นมาเอง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKiqa-pfntgM"
      },
      "source": [
        "### Annotation, Annotation, Annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W7BjeFxGRxR"
      },
      "source": [
        "วิธีที่ตรงไปตรงมาที่สุดคือการเก็บและ label ข้อมูลด้วยมนุษย์ คุณสามารถชวนเพื่อนๆมาทำ หรือแม้แต่จ้างบริษัทมืออาชีพ แน่นอนว่าข้อมูลยิ่งมากยิ่งดี แต่หลายครั้งแค่ 1,000 ตัวอย่างก็พอให้คุณเริ่มทำ ML model ได้แล้ว\n",
        "\n",
        "ยกตัวอย่างเช่น [@wannaphong](https://github.com/wannaphong) ผู้ก่อตั้ง [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp) ผู้เชี่ยวชาญด้านการสร้างชุดข้อมูลด้วยมือ [@wannaphong](https://github.com/wannaphong) สร้าง\n",
        "\n",
        "* [thai-named-entity-recognition-data](https://github.com/PyThaiNLP/thai-named-entity-recognition-data/) - ชุดข้อมูล named entity recognition ที่ต่อมารวมกับชุดข้อมูลอีกชุด (ที่ [@wannaphong](https://github.com/wannaphong) ทำการตรวจสอบคุณภาพด้วยมืออีกรอบ) กลายเป็น [ThaiNER](https://github.com/wannaphong/thai-ner) ที่เป็นหนึ่งใน benchmark ที่ใหญ่ที่สุดสำหรับ NER ภาษาไทย ทดลองใช้ได้ที่ [HuggingFace Hub](https://huggingface.co/pythainlp/thainer-corpus-v2-base-model)\n",
        "\n",
        "* [spelling-check](https://github.com/PyThaiNLP/spelling-check) - ชุดข้อมูลตรวจสอบการสะกดคำภาษาไทยกว่า 1,800 ประโยค; ใช้สำหรับฟังชั่นแก้คำผิดของ [PyThaiNLP](https://github.com/PyThaiNLP/pythainlp)\n",
        "\n",
        "* [Thai-Lao-Parallel-Corpus](https://github.com/PyThaiNLP/Thai-Lao-Parallel-Corpus) - แปลภาษาไทย-ลาวประมาณ 260 ประโยค\n",
        "\n",
        "* [thai-sentence](https://github.com/PyThaiNLP/Thai-sentence) - ประโยคภาษาไทยเขียนเองประมาณ 1,000 ประโยค\n",
        "\n",
        "* [thai-synonym](https://github.com/PyThaiNLP/thai-synonym/) - คำพ้องความหมายประมาณ 100 คำ\n",
        "\n",
        "ข้อดีของการทำแบบนี้คือคุณจะเข้าใจชุดข้อมูลของคุณได้ดียิ่งขึ้น และยิ่งถ้าคุณเปิดเผยชุดข้อมูลเป็นสาธารณะก็อาจจะมีคนมาช่วยเพิ่มข้อมูลให้คุณอีกด้วย"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR96SNtmX69T"
      },
      "source": [
        "### สร้าง Tabular Data เพิ่มด้วย ML Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1ufGZnOnNSO"
      },
      "source": [
        "เริ่มจากถ้าเรามีข้อมูล tabular อยู่บ้าง แต่ต้องการเพิ่มเพื่อให้โมเดลมีประสิทธิภาพขึ้น เราสามารถสร้างชุดข้อมูลเพิ่มด้วยเทคนิค เช่น [CTGAN](https://arxiv.org/abs/1907.00503) เรียกใช้จาก [sdv](https://sdv.dev/SDV/user_guides/single_table/ctgan.html)\n",
        "\n",
        "เราทดลองใช้ [CTGAN](https://arxiv.org/abs/1907.00503) สร้างชุดข้อมูลเพิ่มสำหรับโมเดล [xgboost](https://xgboost.readthedocs.io/en/stable/) ในบทที่ 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WsV0rmSguop"
      },
      "source": [
        "จำแนกการตอบสนองของลูกค้ากับแคมเปญที่บริษัทจัดด้วยชุดข้อมูล [Retail Transaction Data](https://www.kaggle.com/regivm/retailtransactiondata) จัดทำโดย []() ดูตัวอย่างการวิเคราะห์เต็มๆได้ที่ [Campaign Response Model - Example.ipynb](https://colab.research.google.com/drive/1eTVlSYcwUYkYDW6sPTY8XVO1QMhr-DNT?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOaKT9TIX94x"
      },
      "outputs": [],
      "source": [
        "#download data\n",
        "!wget https://github.com/ai-builders/curriculum/raw/main/data/Retail_Data_Transactions.csv\n",
        "!wget https://github.com/ai-builders/curriculum/raw/main/data/Retail_Data_Response.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBkvIkfLgxh2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "#จัดการข้อมูลให้อยู่ในรูปแบบ (X, y)\n",
        "df_response = pd.read_csv('Retail_Data_Response.csv')\n",
        "df_trans = pd.read_csv('Retail_Data_Transactions.csv', parse_dates=['trans_date'])\n",
        "\n",
        "campaign_date = datetime(2015,3,17)\n",
        "df_trans['age'] = (campaign_date - df_trans.trans_date).dt.days\n",
        "\n",
        "df = df_trans.groupby('customer_id').agg(\n",
        "    recency=('age', 'min'),\n",
        "    frequency=('customer_id', 'count'),\n",
        "    monetary=('tran_amount', 'sum'),\n",
        "    tenure=('age', 'max'),\n",
        "    length_of_stay=('age', lambda x: x.max() - x.min())\n",
        ")\n",
        "df['ticket_size'] = df.monetary / df.frequency\n",
        "df['std_ticket_size'] = df_trans.groupby('customer_id').tran_amount.std()\n",
        "df['cv_ticket_size'] = df.std_ticket_size / df.ticket_size\n",
        "\n",
        "# the number of months visited\n",
        "active_months = df_trans.groupby(['customer_id', pd.Grouper(key='trans_date',freq='M')]).size().reset_index()\n",
        "active_months = active_months.groupby('customer_id').trans_date.nunique()\n",
        "df['active_months'] = active_months\n",
        "df['avg_spend_m'] = df.monetary / df.active_months\n",
        "df['avg_visit_m'] = df.frequency / df.active_months\n",
        "# done with active_months, then drop it\n",
        "df = df.drop(columns='active_months')\n",
        "df['avg_tte'] = df.length_of_stay / (df.frequency - 1)\n",
        "\n",
        "#all data\n",
        "all_data = df_response.join(df, how='inner', on='customer_id').set_index('customer_id')\n",
        "\n",
        "#split ข้อมูลเป็น train, validation และ test\n",
        "random_state = 1024\n",
        "test_size = 0.2\n",
        "val_size = 0.2\n",
        "\n",
        "X = all_data.drop(columns='response')\n",
        "y = all_data.response\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                            stratify=y, random_state=random_state)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size),\n",
        "                                                  stratify=y_train_val, random_state=random_state)\n",
        "\n",
        "train_data = X_train.join(y_train)\n",
        "val_data = X_val.join(y_val)\n",
        "test_data = X_test.join(y_test)\n",
        "\n",
        "train_data.shape, val_data.shape, test_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsK0DosnjFP6"
      },
      "source": [
        "เราสร้างชุดข้อมูลเพิ่มจาก `train data` ด้วย CTGAN (ระวังอย่าสร้างจาก validation หรือ test sets ไม่งั้นจะเปิด data leakage (\"โกงข้อสอบ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzrdka9wjI4d"
      },
      "outputs": [],
      "source": [
        "from sdv.tabular import CTGAN\n",
        "\n",
        "#สร้างโมเดล CTGAN เพื่อเรียนรู้จากข้อมูล\n",
        "model = CTGAN()\n",
        "model.fit(train_data)\n",
        "\n",
        "#สร้างชุดข้อมูลใหม่มา 4000 ตัวอย่าง (เดิม training set มี 4,130 ตัวอย่าง)\n",
        "new_train_data = model.sample(4000)\n",
        "new_train_data.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTm4wjcroTeF"
      },
      "outputs": [],
      "source": [
        "#ประเมินว่าสร้างข้อมูลปลอมได้ดีแค่ไหน; ค่ายิ่งสูงยิ่งดี\n",
        "from sdv.evaluation import evaluate\n",
        "\n",
        "evaluate(new_train_data, real_data=train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWm-qUpdjrOY"
      },
      "outputs": [],
      "source": [
        "#สร้าง X_train_new และ y_train_new ด้วยการเพิ่มข้อมูลปลอมเข้าไป\n",
        "X_train_new = pd.concat([X_train,new_train_data.drop(columns='response')])\n",
        "y_train_new = pd.concat([y_train,new_train_data.response])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp4w4klIhBEn"
      },
      "outputs": [],
      "source": [
        "#สร้างโมเดลต้นไม้ด้วย xgboost (gradient boosted trees)\n",
        "import xgboost as xgb\n",
        "\n",
        "#เทรนด้วยข้อมูลจริง\n",
        "xgb_params = {'objective': 'binary:logistic',\n",
        "              'random_state': 1024,\n",
        "              'eval_metric': 'auc',\n",
        "              'early_stopping_rounds': 10,\n",
        "              'scale_pos_weight' : (len(y_train) - sum(y_train)) / sum(y_train),\n",
        "              'max_depth': 3,\n",
        "              'booster':'gbtree',\n",
        "              }\n",
        "\n",
        "model = xgb.XGBClassifier(**xgb_params)\n",
        "model = model.fit(X_train,\n",
        "                  y_train,\n",
        "                  eval_set=[(X_val,y_val)],\n",
        "                  verbose=False)\n",
        "\n",
        "#เทรนด้วยข้อมูลจริง + ข้อมูลปลอม\n",
        "xgb_params_new = {'objective': 'binary:logistic',\n",
        "              'random_state': 1024,\n",
        "              'eval_metric': 'auc',\n",
        "              'early_stopping_rounds': 10,\n",
        "              'scale_pos_weight' : (len(y_train_new) - sum(y_train_new)) / sum(y_train_new),\n",
        "              'max_depth': 3,\n",
        "              'booster':'gbtree',\n",
        "              }\n",
        "\n",
        "model_new = xgb.XGBClassifier(**xgb_params_new)\n",
        "model_new = model_new.fit(X_train_new,\n",
        "                  y_train_new,\n",
        "                  eval_set=[(X_val,y_val)],\n",
        "                  verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YatXu2yZqSi6"
      },
      "source": [
        "จะเห็นได้ว่าเมื่อเพิ่มข้อมูลปลอมเข้าไปแล้ว micro-averaged F1 และ accuracy ดีขึ้น แต่ AUC แย่ลง เพราะฉะนั้นการที่เราจะเพิ่มข้อมูลปลอมเข้าไปเทรนจำเป็นต้องคำนึงถึงว่าเราต้องการวัดผลของโมเดลด้วย metric ใด"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSghpFORhG2w"
      },
      "outputs": [],
      "source": [
        "#รายงานผลโมเดลเทรนบนข้อมูลจริง\n",
        "print(f'AUC: {roc_auc_score(y_test, model.predict_proba(X_test)[:,1])}')\n",
        "print(classification_report(y_test,model.predict_proba(X_test)[:,1].round()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VINSp5uOhG4r"
      },
      "outputs": [],
      "source": [
        "#รายงานผลโมเดลเทรนบนข้อมูลจริง+ข้อมูลปลอม\n",
        "print(f'AUC: {roc_auc_score(y_test, model_new.predict_proba(X_test)[:,1])}')\n",
        "print(classification_report(y_test,model_new.predict_proba(X_test)[:,1].round()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrjCnNwOS3eE"
      },
      "source": [
        "### สร้าง Texts จาก ML Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpc7DeuNERKD"
      },
      "source": [
        "สำหรับโมเดล NLP นั้นโชคดีที่ในอินเตอร์เน็ตมีข้อความมากมายให้นักวิจัยสามารถเทรนโมเดลขนาดใหญ่ได้ฟรี เราจึงมีโมเดลที่สามารถสร้างข้อความในแบบที่เราต้องการให้เลือกใช้ได้อย่างหลากหลายใน [HuggingFace Hub](https://huggingface.co/models)\n",
        "\n",
        "เทคนิคการสร้างข้อความปลอมเพื่อมาเทรนโมเดลนี้เคยถูกใช้สำหรับเทรน[โมเดลแปลภาษาไทย-อังกฤษของ VISTEC](https://airesearch.in.th/releases/machine-translation-models/) ที่สามารถทำผลงานได้ดีที่สุด ณ วันตีพิมพ์ (หาอ่านงานวิจัยได้ที่ [scb-mt-en-th-2020: A Large English-Thai Parallel Corpus](https://arxiv.org/abs/2007.03541)) โดยเราใช้โมเดล [CTRL](https://github.com/salesforce/ctrl) สร้างข้อความรีวิวสินค้าระหว่าง 1-5 ดาวเป็นภาษาอังกฤษออกมาเพื่อให้นักแปลนำไปแปลเพื่อสร้างคู่ประโยคสำหรับเทรนโมเดลอีกที (ดูรายละเอียดได้ที่ [vistec-ai/fake_reviews](https://github.com/vistec-AI/fake_reviews))\n",
        "\n",
        "แน่นอนว่าทุกวันนี้มีโมเดลที่สามารถสร้างข้อความได้แทบทุกรูปแบบ เราจะยกตัวอย่างโมเดล [GPT](https://arxiv.org/abs/2005.14165) สำหรับการสร้างรีวิวสินค้า 1 ดาวและ 5 ดาว"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0BH4yaWLrjw"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pprint\n",
        "\n",
        "#โหลด generator สำหรับรีวิว 1 ดาว\n",
        "generator1 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"ehdwns1516/gpt2_review_star1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-seJg3YNXCg"
      },
      "outputs": [],
      "source": [
        "context = '<|endoftext|>' #โมเดลต้องการ token พิเศษนี้นำหน้าเพื่อสร้างข้อความ\n",
        "pprint.pprint([i['generated_text'] for i in generator1(context,\n",
        "                                                      max_length = 60, #ให้แต่ละประโยคมีความยาว 60 token\n",
        "                                                      num_return_sequences=3 #สร้างมา 3 ประโยค\n",
        "                                                       )])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qu5cZTHODp8"
      },
      "outputs": [],
      "source": [
        "#โหลด generator สำหรับรีวิว 1 ดาว\n",
        "generator5 = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"ehdwns1516/gpt2_review_star5\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q3YGoP6OJrm"
      },
      "outputs": [],
      "source": [
        "context = '<|endoftext|>' #โมเดลต้องการ token พิเศษนี้นำหน้าเพื่อสร้างข้อความ\n",
        "pprint.pprint([i['generated_text'] for i in generator5(context,\n",
        "                                                      max_length = 60, #ให้แต่ละประโยคมีความยาว 60 token\n",
        "                                                      num_return_sequences=3 #สร้างมา 3 ประโยค\n",
        "                                                       )])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXOSHy3PSy3x"
      },
      "source": [
        "### แปลชุดข้อมูลภาษาหลัก"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ns_0_dCstuV"
      },
      "source": [
        "อีกทางหนึ่งสำหรับการหาข้อมูลข้อความ โดยเฉพาะเป็นภาษาไทย คือคุณสามารถหาชุดข้อมูลภาษาหลัก เช่น อังกฤษหรือจีน มาแปลเป็นไทยได้ด้วยโมเดล open source เช่น `pythainlp.translate`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNbNzNoP2AO0"
      },
      "outputs": [],
      "source": [
        "#machine translation\n",
        "!pip install -q fairseq==0.10.0\n",
        "!pip install -q pythainlp[translate]==3.1.1 sentencepiece==0.1.97\n",
        "!pip install -q torch==1.8.0 #downgrade torch สำหรับแปลไทย-จีน\n",
        "exit() #reset runtime เพื่อให้ package ใช้งานได้อย่างถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQYf6HZYQxN7"
      },
      "outputs": [],
      "source": [
        "from pythainlp.translate import Translate\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "th2en = Translate('th', 'en')\n",
        "en2th = Translate('en', 'th')\n",
        "\n",
        "th2zh = Translate('th', 'zh')\n",
        "zh2th = Translate('zh', 'th')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTEnRsDuRoif"
      },
      "outputs": [],
      "source": [
        "th2en.translate(['ฉันชอบกินพิซซ่า', 'เธอไม่อยากเล่นฟุตบอล'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o64mn058dwD"
      },
      "outputs": [],
      "source": [
        "en2th.translate(['I like pizza.', \"She doesn't want to play football.\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOO4rjLuTKAU"
      },
      "outputs": [],
      "source": [
        "[th2zh.translate(i) for i in ['ฉันชอบกินพิซซ่า', 'เธอไม่อยากเล่นฟุตบอล']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRwPiMv08sww"
      },
      "outputs": [],
      "source": [
        "[zh2th.translate(i) for i in ['我喜欢吃披萨。', '她不想踢足球。']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY5sEv0cq2IJ"
      },
      "source": [
        "### สร้าง Images จาก ML Models\n",
        "\n",
        "เช่นเดียวกับข้อมูลข้อความ โมเดลขนาดใหญ่ที่ถูกเทรนด้วยรูปภาพมหาศาลก็สามารถสร้างรูปภาพปลอมเพื่อให้คุณไปเทรนโมเดลได้เช่นกัน เช่น [DALL-E Mini](https://github.com/borisdayma/dalle-mini) ที่เป็นโมเดล open source สร้างเลียนแบบโมเดล [DALL-E](https://openai.com/blog/dall-e/) ของ [OpenAI](https://openai.com/)\n",
        "\n",
        "วิธีการสร้างรูปภาพค่อนข้างซับซ้อนและกินเวลานาน แนะนำให้ไปลองเล่นกันที่ [demo](https://huggingface.co/spaces/flax-community/dalle-mini) หรือดูรายละเอียดที่ [github repository](https://github.com/borisdayma/dalle-mini)\n",
        "\n",
        "<img src=https://github.com/ai-builders/curriculum/raw/main/images/dalle_e_toast.png width=\"700px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-7BZmoOwJt1"
      },
      "source": [
        "อีกตัวอย่างใกล้ตัวคือการใช้ Generative Adversarial Networks (GAN) เรียนรู้จากรูปที่เรามีอยู่เพื่อสร้างตัวอย่างเพิ่มขึ้น เช่น การสร้าง Anime faces จากรูปที่มีอยู่โดย [@konkuad](https://github.com/konkuad) ใน [konkuad/GANime](https://github.com/konkuad/GANime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsOnvu9AvsmB"
      },
      "outputs": [],
      "source": [
        "!rm -r GANime\n",
        "!git clone https://github.com/konkuad/GANime\n",
        "!cd GANime; pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDTgteRNv1g0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from GANime.gan import GAN\n",
        "\n",
        "#initialize new model with random weights\n",
        "seed_size=128\n",
        "new_gan_model = GAN(seed_size)\n",
        "#load model trained for 100 epochs\n",
        "new_gan_model.gen.load_state_dict(torch.load('GANime/example_models/gen.pt'))\n",
        "new_gan_model.dis.load_state_dict(torch.load('GANime/example_models/dis.pt'))\n",
        "#infer generation\n",
        "generated_images = new_gan_model.generate(num_rows=8,\n",
        "               num_cols=8,\n",
        "               plot=True,\n",
        "               device='cpu',\n",
        "               return_noise=False\n",
        "              )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSd7_36FpQxC"
      },
      "source": [
        "# Checkpoint ท้ายบท"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlbWhnAVGpOz"
      },
      "source": [
        "## คำถามชวนคิดเกี่ยวกับบทเรียน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmoDJh8TGqB3"
      },
      "source": [
        "1. คุณคิดว่าปัญหาที่คุณกำลังจะแก้ด้วย ML นั้นปริมาณและคุณภาพของข้อมูลที่ใช้เทรน (Data-Centric AI) หรือประสิทธิภาพของโมเดล (Model-Centric AI) สำคัญกว่ากัน เพราะอะไร\n",
        "\n",
        "2. คุณคิดว่า open data และ open source มีข้อดี-ข้อเสียอย่างไรต่อ 1) ผู้สร้างผลงาน 2) ผู้นำผลงานไปใช้ 3) ชุมชน 4) สังคมโดยรวม\n",
        "\n",
        "3. คุณคิดว่าการใช้ข้อมูลสาธารณะจากเว็บไซต์ต่างๆมาเทรนโมเดลมีข้อควรระวังอะไรบ้าง\n",
        "\n",
        "4. การเทรนโมเดลด้วยข้อมูลที่สร้างขึ้นมาจากอีกโมเดลหนึ่ง (\"ข้อมูลปลอม\") นั้นมีข้อดี-ข้อเสียอย่างไรบ้าง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzKovFviehas"
      },
      "source": [
        "## สิ่งที่ควรเตรียมพร้อมสำหรับทำโครงงาน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQC0Vmqfh7Dn"
      },
      "source": [
        "### ☑️ ทบทวนระบบการให้คะแนนโครงงานให้เรียบร้อย\n",
        "\n",
        "AI Builders จะออกใบประกาศนียบัตรจบการศึกษาให้กับผู้เข้าร่วมโครงการที่ส่งโครงงานได้คะแนนอย่างน้อย 70 จาก 100 คะแนนตามเกณฑ์ต่อไปนี้เท่านั้น\n",
        "\n",
        "1. problem statement; เหตุผลในการแก้ปัญหาเชิงธุรกิจ/ชีวิตประจำวันด้วย machine learning - 15 คะแนน\n",
        "2. metrics and baselines; การให้เหตุผลเชื่อมโยงการแก้ปัญหากับตัวชี้วัดที่เลือก / การวัดผลเทียบกับวิธีแก้ปัญหาในปัจจุบัน - 15 คะแนน\n",
        "\n",
        "**วันนี้เราจะคิดถึง 2 ข้อนี้เป็นพิเศษ**\n",
        "\n",
        "**3. data collection and cleaning; การเก็บและทำความสะอาดข้อมูล - 15 คะแนน**\n",
        "\n",
        "\n",
        "**4. exploratory data analysis; การทำความเข้าใจข้อมูล - 20 คะแนน**\n",
        "\n",
        "\n",
        "5. modeling, validation and error analysis; การทำโมเดล, ทดสอบโมเดล และวิเคราะห์ข้อผิดพลาดของโมเดล - 20 คะแนน\n",
        "6. deployment; การนำโมเดลไปใช้แก้ปัญหาจริง - 15 คะแนน"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfkgDraIzfq"
      },
      "source": [
        "### ☑️ หาข้อมูลมาจากไหนดี\n",
        "\n",
        "จากแหล่งข้อมูลทั้งหมดที่กล่าวมาในบทเรียนนี้ คุณคิดว่าจะหาชุดข้อมูลจากไหนมาเทรนโมเดล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CooO6inerF5a"
      },
      "source": [
        "### ☑️ คุณภาพของข้อมูล Labels และ Inputs\n",
        "\n",
        "ข้อมูลที่คุณได้มาจากอินเตอร์เน็ตไม่ว่าจะดาวน์โหลดชุดข้อมูลของคนอื่น, scrape เว็บไซต์, ใช้โมเดลคนอื่นสร้างขึ้นมา คุณต้องทำให้แน่ใจว่าคุณภาพมันพร้อมใช้งานทั้ง Labels และ Inputs เช่น ถ้าอยากทำโมเดลแปลภาษาไทย-จีนก็ต้องทำให้แน่ใจว่าคู่ประโยคที่เราหามามันแปลตรงกันจริงๆ เป็นต้น"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCHkJExek5w"
      },
      "source": [
        "### ☑️ Train-validation-test Splits สมเหตุสมผลไหม\n",
        "\n",
        "ใช้สามัญสำนึกและตรรกะในการแบ่งข้อมูลเป็น train, validation, test sets ทำให้มั่นใจว่าไม่มีตัวอย่างเดียวกันหลุดไปใน set อื่น เช่น\n",
        "\n",
        "* ในการทำ face recognition ไม่ควรมีรูปคนๆเดียวกันในมากกว่า 1 set\n",
        "\n",
        "* หากทำ time series forecasting เรียงข้อมูลจากเก่าสุดไปใหม่สุดใน `train`, `validation` และ `test` เพื่อไม่ให้โมเดลเห็นข้อมูลจากอนาคต\n",
        "\n",
        "* หากทำ product recommendation ไม่ควรมีข้อมูลของลูกค้าคนเดียวกันในมากกว่า 1 set เพราะจะทำให้เราเดาว่าเขาเป็นลูกค้าประเภทไหนได้ด้วยข้อมูลที่ไม่ควรเห็น ฯลฯ\n",
        "\n",
        "ทั้งนี้ขึ้นอยู่กับวิจารณญาณของผู้สร้างโมเดล เรียนรู้เพิ่มเติมเกี่ยวกับการ split ข้อมูลได้จาก เรียนรู้เพิ่มเติมเกี่ยวกับ metric ได้จาก [DS&AI Academy](https://www.youtube.com/playlist?list=PL1kutgc5YvC841TmatsLuj4sDKkwj95Dz)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "02_fantastic_datasets.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}